2023-03-17 07:40:12,379:INFO: Effective parameters:
2023-03-17 07:40:12,379:INFO:   <<< batch_size: 64
2023-03-17 07:40:12,380:INFO:   <<< batch_size_val: 8
2023-03-17 07:40:12,380:INFO:   <<< cache_dir: 
2023-03-17 07:40:12,380:INFO:   <<< coef_lr: 0.001
2023-03-17 07:40:12,380:INFO:   <<< cross_model: cross-base
2023-03-17 07:40:12,380:INFO:   <<< cross_num_hidden_layers: 4
2023-03-17 07:40:12,380:INFO:   <<< data_path: ../dataset/MSRVTT/MSRVTT_data.json
2023-03-17 07:40:12,380:INFO:   <<< datatype: msrvtt
2023-03-17 07:40:12,380:INFO:   <<< do_eval: False
2023-03-17 07:40:12,380:INFO:   <<< do_lower_case: False
2023-03-17 07:40:12,380:INFO:   <<< do_pretrain: False
2023-03-17 07:40:12,380:INFO:   <<< do_train: True
2023-03-17 07:40:12,380:INFO:   <<< epochs: 5
2023-03-17 07:40:12,380:INFO:   <<< eval_frame_order: 0
2023-03-17 07:40:12,380:INFO:   <<< expand_msrvtt_sentences: True
2023-03-17 07:40:12,380:INFO:   <<< feature_framerate: 1
2023-03-17 07:40:12,380:INFO:   <<< features_path: ../dataset/MSRVTT/MSRVTT_Videos
2023-03-17 07:40:12,380:INFO:   <<< fp16: False
2023-03-17 07:40:12,380:INFO:   <<< fp16_opt_level: O1
2023-03-17 07:40:12,380:INFO:   <<< freeze_layer_num: 0
2023-03-17 07:40:12,380:INFO:   <<< gradient_accumulation_steps: 1
2023-03-17 07:40:12,380:INFO:   <<< hard_negative_rate: 0.5
2023-03-17 07:40:12,380:INFO:   <<< init_model: None
2023-03-17 07:40:12,380:INFO:   <<< linear_patch: 2d
2023-03-17 07:40:12,380:INFO:   <<< local_rank: 0
2023-03-17 07:40:12,380:INFO:   <<< loose_type: True
2023-03-17 07:40:12,380:INFO:   <<< lr: 0.0001
2023-03-17 07:40:12,380:INFO:   <<< lr_decay: 0.9
2023-03-17 07:40:12,380:INFO:   <<< margin: 0.1
2023-03-17 07:40:12,380:INFO:   <<< max_frames: 12
2023-03-17 07:40:12,380:INFO:   <<< max_words: 32
2023-03-17 07:40:12,380:INFO:   <<< n_display: 50
2023-03-17 07:40:12,380:INFO:   <<< n_gpu: 1
2023-03-17 07:40:12,380:INFO:   <<< n_pair: 1
2023-03-17 07:40:12,380:INFO:   <<< negative_weighting: 1
2023-03-17 07:40:12,380:INFO:   <<< num_thread_reader: 0
2023-03-17 07:40:12,380:INFO:   <<< output_dir: ckpts/ckpt_msrvtt_retrieval_looseType
2023-03-17 07:40:12,380:INFO:   <<< pretrained_clip_name: ViT-B/32
2023-03-17 07:40:12,380:INFO:   <<< rank: 0
2023-03-17 07:40:12,380:INFO:   <<< resume_model: None
2023-03-17 07:40:12,380:INFO:   <<< sampled_use_mil: False
2023-03-17 07:40:12,380:INFO:   <<< seed: 42
2023-03-17 07:40:12,380:INFO:   <<< sim_header: meanP
2023-03-17 07:40:12,380:INFO:   <<< slice_framepos: 2
2023-03-17 07:40:12,380:INFO:   <<< task_type: retrieval
2023-03-17 07:40:12,380:INFO:   <<< text_num_hidden_layers: 12
2023-03-17 07:40:12,380:INFO:   <<< train_csv: ../dataset/MSRVTT/MSRVTT_train.9k.csv
2023-03-17 07:40:12,381:INFO:   <<< train_frame_order: 0
2023-03-17 07:40:12,381:INFO:   <<< use_mil: False
2023-03-17 07:40:12,381:INFO:   <<< val_csv: ../dataset/MSRVTT/MSRVTT_JSFUSION_test.csv
2023-03-17 07:40:12,381:INFO:   <<< video_dim: 1024
2023-03-17 07:40:12,381:INFO:   <<< visual_num_hidden_layers: 12
2023-03-17 07:40:12,381:INFO:   <<< warmup_proportion: 0.1
2023-03-17 07:40:12,381:INFO:   <<< world_size: 1
2023-03-17 07:40:12,381:INFO: device: cuda:0 n_gpu: 1
2023-03-17 07:40:12,887:INFO: loading archive file /media/dc/新加卷/dc/CLIP4Clip-master/modules/cross-base
2023-03-17 07:40:12,887:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-03-17 07:40:12,888:INFO: Weight doesn't exsits. /media/dc/新加卷/dc/CLIP4Clip-master/modules/cross-base/cross_pytorch_model.bin
2023-03-17 07:40:12,888:WARNING: Stage-One:True, Stage-Two:False
2023-03-17 07:40:12,888:WARNING: Test retrieval by loose type.
2023-03-17 07:40:12,888:WARNING: 	 embed_dim: 512
2023-03-17 07:40:12,888:WARNING: 	 image_resolution: 224
2023-03-17 07:40:12,888:WARNING: 	 vision_layers: 12
2023-03-17 07:40:12,888:WARNING: 	 vision_width: 768
2023-03-17 07:40:12,888:WARNING: 	 vision_patch_size: 32
2023-03-17 07:40:12,888:WARNING: 	 context_length: 77
2023-03-17 07:40:12,888:WARNING: 	 vocab_size: 49408
2023-03-17 07:40:12,888:WARNING: 	 transformer_width: 512
2023-03-17 07:40:12,888:WARNING: 	 transformer_heads: 8
2023-03-17 07:40:12,888:WARNING: 	 transformer_layers: 12
2023-03-17 07:40:12,888:WARNING: 		 linear_patch: 2d
2023-03-17 07:40:12,888:WARNING: 	 cut_top_layer: 0
2023-03-17 07:40:13,616:WARNING: 	 sim_header: meanP
2023-03-17 07:40:16,650:INFO: --------------------
2023-03-17 07:40:16,651:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2023-03-17 07:40:16,693:INFO: ***** Running test *****
2023-03-17 07:40:16,693:INFO:   Num examples = 1000
2023-03-17 07:40:16,693:INFO:   Batch size = 8
2023-03-17 07:40:16,693:INFO:   Num steps = 125
2023-03-17 07:40:16,693:INFO: ***** Running val *****
2023-03-17 07:40:16,693:INFO:   Num examples = 1000
2023-03-17 07:40:21,977:INFO: ***** Running training *****
2023-03-17 07:40:21,978:INFO:   Num examples = 180000
2023-03-17 07:40:21,978:INFO:   Batch size = 64
2023-03-17 07:40:21,978:INFO:   Num steps = 14060
2023-03-17 07:48:39,583:INFO: Epoch: 1/5, Step: 50/2812, Lr: 0.000000004, Loss: 1.196686, Time/step: 9.952093
2023-03-17 07:57:03,145:INFO: Epoch: 1/5, Step: 100/2812, Lr: 0.000000007, Loss: 1.390616, Time/step: 10.071246
2023-03-17 08:05:24,330:INFO: Epoch: 1/5, Step: 150/2812, Lr: 0.000000011, Loss: 1.443472, Time/step: 10.023684
2023-03-17 08:13:42,836:INFO: Epoch: 1/5, Step: 200/2812, Lr: 0.000000014, Loss: 1.513280, Time/step: 9.970114
2023-03-17 08:22:06,866:INFO: Epoch: 1/5, Step: 250/2812, Lr: 0.000000018, Loss: 0.999193, Time/step: 10.080597
2023-03-17 08:30:28,908:INFO: Epoch: 1/5, Step: 300/2812, Lr: 0.000000021, Loss: 0.707438, Time/step: 10.040819
2023-03-17 08:38:45,471:INFO: Epoch: 1/5, Step: 350/2812, Lr: 0.000000025, Loss: 0.873792, Time/step: 9.931254
2023-03-17 08:47:08,773:INFO: Epoch: 1/5, Step: 400/2812, Lr: 0.000000028, Loss: 0.963568, Time/step: 10.066034
2023-03-17 08:55:30,755:INFO: Epoch: 1/5, Step: 450/2812, Lr: 0.000000032, Loss: 0.819758, Time/step: 10.039630
2023-03-17 09:03:51,160:INFO: Epoch: 1/5, Step: 500/2812, Lr: 0.000000036, Loss: 0.809101, Time/step: 10.008098
2023-03-17 09:12:13,405:INFO: Epoch: 1/5, Step: 550/2812, Lr: 0.000000039, Loss: 0.606986, Time/step: 10.044895
2023-03-17 09:20:31,139:INFO: Epoch: 1/5, Step: 600/2812, Lr: 0.000000043, Loss: 1.130995, Time/step: 9.954666
2023-03-17 09:28:43,508:INFO: Epoch: 1/5, Step: 650/2812, Lr: 0.000000046, Loss: 0.785899, Time/step: 9.847375
2023-03-17 09:37:11,677:INFO: Epoch: 1/5, Step: 700/2812, Lr: 0.000000050, Loss: 0.849400, Time/step: 10.163386
2023-03-17 09:45:30,140:INFO: Epoch: 1/5, Step: 750/2812, Lr: 0.000000053, Loss: 0.506009, Time/step: 9.969249
2023-03-17 09:53:48,743:INFO: Epoch: 1/5, Step: 800/2812, Lr: 0.000000057, Loss: 0.549330, Time/step: 9.972039
2023-03-17 10:02:08,341:INFO: Epoch: 1/5, Step: 850/2812, Lr: 0.000000060, Loss: 0.650050, Time/step: 9.991963
2023-03-17 10:10:23,299:INFO: Epoch: 1/5, Step: 900/2812, Lr: 0.000000064, Loss: 0.617058, Time/step: 9.899160
2023-03-17 10:18:39,812:INFO: Epoch: 1/5, Step: 950/2812, Lr: 0.000000068, Loss: 0.738741, Time/step: 9.930249
2023-03-17 10:27:02,171:INFO: Epoch: 1/5, Step: 1000/2812, Lr: 0.000000071, Loss: 0.931809, Time/step: 10.047177
2023-03-17 10:35:27,059:INFO: Epoch: 1/5, Step: 1050/2812, Lr: 0.000000075, Loss: 0.837366, Time/step: 10.097747
2023-03-17 10:43:43,924:INFO: Epoch: 1/5, Step: 1100/2812, Lr: 0.000000078, Loss: 0.526161, Time/step: 9.937294
2023-03-17 10:51:59,427:INFO: Epoch: 1/5, Step: 1150/2812, Lr: 0.000000082, Loss: 0.432983, Time/step: 9.910059
2023-03-17 11:00:15,378:INFO: Epoch: 1/5, Step: 1200/2812, Lr: 0.000000085, Loss: 0.688683, Time/step: 9.919014
2023-03-17 11:08:37,585:INFO: Epoch: 1/5, Step: 1250/2812, Lr: 0.000000089, Loss: 0.485054, Time/step: 10.044128
2023-03-17 11:17:01,248:INFO: Epoch: 1/5, Step: 1300/2812, Lr: 0.000000092, Loss: 1.075590, Time/step: 10.073259
2023-03-17 11:25:31,226:INFO: Epoch: 1/5, Step: 1350/2812, Lr: 0.000000096, Loss: 0.723259, Time/step: 10.199540
2023-03-17 11:33:51,975:INFO: Epoch: 1/5, Step: 1400/2812, Lr: 0.000000100, Loss: 0.725288, Time/step: 10.014978
2023-03-17 11:42:11,909:INFO: Epoch: 1/5, Step: 1450/2812, Lr: 0.000000097, Loss: 0.902449, Time/step: 9.998687
2023-03-17 11:50:33,129:INFO: Epoch: 1/5, Step: 1500/2812, Lr: 0.000000097, Loss: 0.696462, Time/step: 10.024387
2023-03-17 11:58:46,987:INFO: Epoch: 1/5, Step: 1550/2812, Lr: 0.000000097, Loss: 1.015861, Time/step: 9.877146
2023-03-17 12:07:00,820:INFO: Epoch: 1/5, Step: 1600/2812, Lr: 0.000000097, Loss: 0.700241, Time/step: 9.876663
2023-03-17 12:15:21,272:INFO: Epoch: 1/5, Step: 1650/2812, Lr: 0.000000097, Loss: 0.638917, Time/step: 10.009024
2023-03-17 12:23:42,704:INFO: Epoch: 1/5, Step: 1700/2812, Lr: 0.000000096, Loss: 0.481298, Time/step: 10.028627
2023-03-17 12:32:06,973:INFO: Epoch: 1/5, Step: 1750/2812, Lr: 0.000000096, Loss: 0.479955, Time/step: 10.085368
2023-03-17 12:40:37,861:INFO: Epoch: 1/5, Step: 1800/2812, Lr: 0.000000096, Loss: 0.593159, Time/step: 10.217771
2023-03-17 12:48:57,726:INFO: Epoch: 1/5, Step: 1850/2812, Lr: 0.000000096, Loss: 0.553046, Time/step: 9.997282
2023-03-17 12:57:17,699:INFO: Epoch: 1/5, Step: 1900/2812, Lr: 0.000000096, Loss: 0.599597, Time/step: 9.999455
2023-03-17 13:05:38,378:INFO: Epoch: 1/5, Step: 1950/2812, Lr: 0.000000095, Loss: 0.453057, Time/step: 10.013564
2023-03-17 13:13:59,937:INFO: Epoch: 1/5, Step: 2000/2812, Lr: 0.000000095, Loss: 0.849092, Time/step: 10.031184
2023-03-17 13:22:20,081:INFO: Epoch: 1/5, Step: 2050/2812, Lr: 0.000000095, Loss: 0.774484, Time/step: 10.002874
2023-03-17 13:30:37,035:INFO: Epoch: 1/5, Step: 2100/2812, Lr: 0.000000095, Loss: 0.605304, Time/step: 9.939080
2023-03-17 13:38:54,343:INFO: Epoch: 1/5, Step: 2150/2812, Lr: 0.000000094, Loss: 0.745789, Time/step: 9.946146
2023-03-17 13:47:16,135:INFO: Epoch: 1/5, Step: 2200/2812, Lr: 0.000000094, Loss: 0.533653, Time/step: 10.035840
2023-03-17 13:55:41,610:INFO: Epoch: 1/5, Step: 2250/2812, Lr: 0.000000094, Loss: 0.476289, Time/step: 10.109480
2023-03-17 14:04:02,465:INFO: Epoch: 1/5, Step: 2300/2812, Lr: 0.000000094, Loss: 0.449505, Time/step: 10.017086
2023-03-17 14:12:23,496:INFO: Epoch: 1/5, Step: 2350/2812, Lr: 0.000000093, Loss: 0.668324, Time/step: 10.020617
2023-03-17 14:20:48,245:INFO: Epoch: 1/5, Step: 2400/2812, Lr: 0.000000093, Loss: 0.415471, Time/step: 10.094980
2023-03-17 14:29:08,865:INFO: Epoch: 1/5, Step: 2450/2812, Lr: 0.000000093, Loss: 0.570814, Time/step: 10.012384
2023-03-17 14:37:32,150:INFO: Epoch: 1/5, Step: 2500/2812, Lr: 0.000000092, Loss: 0.573234, Time/step: 10.065699
2023-03-17 14:45:50,397:INFO: Epoch: 1/5, Step: 2550/2812, Lr: 0.000000092, Loss: 0.632915, Time/step: 9.964942
2023-03-17 14:54:07,674:INFO: Epoch: 1/5, Step: 2600/2812, Lr: 0.000000092, Loss: 0.430933, Time/step: 9.945535
2023-03-17 15:02:27,156:INFO: Epoch: 1/5, Step: 2650/2812, Lr: 0.000000091, Loss: 0.386801, Time/step: 9.989630
2023-03-17 15:10:48,585:INFO: Epoch: 1/5, Step: 2700/2812, Lr: 0.000000091, Loss: 0.370318, Time/step: 10.028561
2023-03-17 15:19:04,732:INFO: Epoch: 1/5, Step: 2750/2812, Lr: 0.000000091, Loss: 0.512120, Time/step: 9.922937
2023-03-17 15:27:26,747:INFO: Epoch: 1/5, Step: 2800/2812, Lr: 0.000000091, Loss: 0.243211, Time/step: 10.040295
2023-03-17 15:29:28,830:INFO: Epoch 1/5 Finished, Train Loss: 0.722559
2023-03-17 15:29:31,071:INFO: Model saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0
2023-03-17 15:29:31,071:INFO: Optimizer saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.0
2023-03-17 15:32:06,237:INFO: sim matrix size: 1000, 1000
2023-03-17 15:32:06,296:INFO: 	 Length-T: 1000, Length-V:1000
2023-03-17 15:32:06,296:INFO: Text-to-Video:
2023-03-17 15:32:06,296:INFO: 	>>>  R@1: 41.8 - R@5: 69.0 - R@10: 79.7 - Median R: 2.0 - Mean R: 16.1
2023-03-17 15:32:06,296:INFO: Video-to-Text:
2023-03-17 15:32:06,296:INFO: 	>>>  V2T$R@1: 41.7 - V2T$R@5: 68.8 - V2T$R@10: 80.4 - V2T$Median R: 2.0 - V2T$Mean R: 12.2
2023-03-17 15:32:06,297:INFO: The best model is: ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0, the R1 is: 41.8000
2023-03-17 15:38:25,495:INFO: Epoch: 2/5, Step: 38/2812, Lr: 0.000000090, Loss: 0.305169, Time/step: 7.580683
2023-03-17 15:46:43,288:INFO: Epoch: 2/5, Step: 88/2812, Lr: 0.000000090, Loss: 0.415251, Time/step: 9.955856
2023-03-17 15:54:57,566:INFO: Epoch: 2/5, Step: 138/2812, Lr: 0.000000090, Loss: 0.426376, Time/step: 9.885541
2023-03-17 16:03:13,107:INFO: Epoch: 2/5, Step: 188/2812, Lr: 0.000000089, Loss: 0.681677, Time/step: 9.910812
2023-03-17 16:11:35,496:INFO: Epoch: 2/5, Step: 238/2812, Lr: 0.000000089, Loss: 0.440598, Time/step: 10.047769
2023-03-17 16:19:53,810:INFO: Epoch: 2/5, Step: 288/2812, Lr: 0.000000088, Loss: 0.308347, Time/step: 9.966275
2023-03-17 16:28:14,179:INFO: Epoch: 2/5, Step: 338/2812, Lr: 0.000000088, Loss: 0.590050, Time/step: 10.007381
2023-03-17 16:36:36,278:INFO: Epoch: 2/5, Step: 388/2812, Lr: 0.000000088, Loss: 0.639876, Time/step: 10.041968
2023-03-17 16:45:00,333:INFO: Epoch: 2/5, Step: 438/2812, Lr: 0.000000087, Loss: 0.545869, Time/step: 10.081095
2023-03-17 16:53:20,233:INFO: Epoch: 2/5, Step: 488/2812, Lr: 0.000000087, Loss: 0.359455, Time/step: 9.997990
2023-03-17 17:01:38,376:INFO: Epoch: 2/5, Step: 538/2812, Lr: 0.000000087, Loss: 0.824802, Time/step: 9.962860
2023-03-17 17:10:12,116:INFO: Epoch: 2/5, Step: 588/2812, Lr: 0.000000086, Loss: 0.230443, Time/step: 10.274786
2023-03-17 17:18:38,180:INFO: Epoch: 2/5, Step: 638/2812, Lr: 0.000000086, Loss: 0.396982, Time/step: 10.121278
2023-03-17 17:26:53,847:INFO: Epoch: 2/5, Step: 688/2812, Lr: 0.000000085, Loss: 0.599257, Time/step: 9.913317
2023-03-17 17:35:18,401:INFO: Epoch: 2/5, Step: 738/2812, Lr: 0.000000085, Loss: 0.607722, Time/step: 10.091068
2023-03-17 17:43:34,360:INFO: Epoch: 2/5, Step: 788/2812, Lr: 0.000000085, Loss: 0.456263, Time/step: 9.919181
2023-03-17 17:51:52,283:INFO: Epoch: 2/5, Step: 838/2812, Lr: 0.000000084, Loss: 0.333489, Time/step: 9.958452
2023-03-17 18:00:15,765:INFO: Epoch: 2/5, Step: 888/2812, Lr: 0.000000084, Loss: 0.416160, Time/step: 10.069625
2023-03-17 18:08:38,549:INFO: Epoch: 2/5, Step: 938/2812, Lr: 0.000000083, Loss: 0.474889, Time/step: 10.055688
2023-03-17 18:17:03,864:INFO: Epoch: 2/5, Step: 988/2812, Lr: 0.000000083, Loss: 0.591741, Time/step: 10.106298
2023-03-17 18:25:21,213:INFO: Epoch: 2/5, Step: 1038/2812, Lr: 0.000000083, Loss: 0.230349, Time/step: 9.946963
2023-03-17 18:33:41,055:INFO: Epoch: 2/5, Step: 1088/2812, Lr: 0.000000082, Loss: 0.321343, Time/step: 9.996837
2023-03-17 18:42:03,715:INFO: Epoch: 2/5, Step: 1138/2812, Lr: 0.000000082, Loss: 0.350890, Time/step: 10.053191
2023-03-17 18:50:27,055:INFO: Epoch: 2/5, Step: 1188/2812, Lr: 0.000000081, Loss: 0.270112, Time/step: 10.066794
2023-03-17 18:58:46,988:INFO: Epoch: 2/5, Step: 1238/2812, Lr: 0.000000081, Loss: 0.320365, Time/step: 9.998650
2023-03-17 19:07:09,807:INFO: Epoch: 2/5, Step: 1288/2812, Lr: 0.000000080, Loss: 0.299657, Time/step: 10.056374
2023-03-17 19:15:38,130:INFO: Epoch: 2/5, Step: 1338/2812, Lr: 0.000000080, Loss: 0.466122, Time/step: 10.166456
2023-03-17 19:23:58,273:INFO: Epoch: 2/5, Step: 1388/2812, Lr: 0.000000080, Loss: 0.482841, Time/step: 10.002861
2023-03-17 19:32:11,459:INFO: Epoch: 2/5, Step: 1438/2812, Lr: 0.000000079, Loss: 0.336396, Time/step: 9.863710
2023-03-17 19:40:33,186:INFO: Epoch: 2/5, Step: 1488/2812, Lr: 0.000000079, Loss: 0.375723, Time/step: 10.034518
2023-03-17 19:48:56,387:INFO: Epoch: 2/5, Step: 1538/2812, Lr: 0.000000078, Loss: 0.422544, Time/step: 10.064014
2023-03-17 19:57:17,449:INFO: Epoch: 2/5, Step: 1588/2812, Lr: 0.000000078, Loss: 0.250508, Time/step: 10.021245
2023-03-17 20:05:43,328:INFO: Epoch: 2/5, Step: 1638/2812, Lr: 0.000000077, Loss: 0.651007, Time/step: 10.117573
2023-03-17 20:14:05,900:INFO: Epoch: 2/5, Step: 1688/2812, Lr: 0.000000077, Loss: 0.671582, Time/step: 10.051433
2023-03-17 20:22:31,448:INFO: Epoch: 2/5, Step: 1738/2812, Lr: 0.000000076, Loss: 0.189213, Time/step: 10.110952
2023-03-17 20:30:54,872:INFO: Epoch: 2/5, Step: 1788/2812, Lr: 0.000000076, Loss: 0.695486, Time/step: 10.068464
2023-03-17 20:39:17,539:INFO: Epoch: 2/5, Step: 1838/2812, Lr: 0.000000075, Loss: 0.342928, Time/step: 10.053336
2023-03-17 20:47:36,410:INFO: Epoch: 2/5, Step: 1888/2812, Lr: 0.000000075, Loss: 0.621538, Time/step: 9.977420
2023-03-17 20:56:00,699:INFO: Epoch: 2/5, Step: 1938/2812, Lr: 0.000000074, Loss: 0.296793, Time/step: 10.085758
2023-03-17 21:04:20,534:INFO: Epoch: 2/5, Step: 1988/2812, Lr: 0.000000074, Loss: 0.421079, Time/step: 9.996699
2023-03-17 21:12:37,357:INFO: Epoch: 2/5, Step: 2038/2812, Lr: 0.000000073, Loss: 0.404417, Time/step: 9.936459
2023-03-17 21:21:02,023:INFO: Epoch: 2/5, Step: 2088/2812, Lr: 0.000000073, Loss: 0.541448, Time/step: 10.093304
2023-03-17 21:29:26,088:INFO: Epoch: 2/5, Step: 2138/2812, Lr: 0.000000072, Loss: 0.293321, Time/step: 10.081293
2023-03-17 21:37:43,105:INFO: Epoch: 2/5, Step: 2188/2812, Lr: 0.000000072, Loss: 0.438128, Time/step: 9.940332
2023-03-17 21:46:07,725:INFO: Epoch: 2/5, Step: 2238/2812, Lr: 0.000000071, Loss: 0.656357, Time/step: 10.092399
2023-03-17 21:54:34,456:INFO: Epoch: 2/5, Step: 2288/2812, Lr: 0.000000071, Loss: 0.359238, Time/step: 10.134608
2023-03-17 22:02:54,911:INFO: Epoch: 2/5, Step: 2338/2812, Lr: 0.000000070, Loss: 0.398995, Time/step: 10.009093
2023-03-17 22:11:10,501:INFO: Epoch: 2/5, Step: 2388/2812, Lr: 0.000000070, Loss: 0.323564, Time/step: 9.911782
2023-03-17 22:19:33,299:INFO: Epoch: 2/5, Step: 2438/2812, Lr: 0.000000069, Loss: 0.259276, Time/step: 10.055963
2023-03-17 22:27:57,827:INFO: Epoch: 2/5, Step: 2488/2812, Lr: 0.000000069, Loss: 0.288410, Time/step: 10.090558
2023-03-17 22:36:19,979:INFO: Epoch: 2/5, Step: 2538/2812, Lr: 0.000000068, Loss: 0.358816, Time/step: 10.043040
2023-03-17 22:44:43,789:INFO: Epoch: 2/5, Step: 2588/2812, Lr: 0.000000068, Loss: 0.498748, Time/step: 10.076185
2023-03-17 22:52:59,968:INFO: Epoch: 2/5, Step: 2638/2812, Lr: 0.000000067, Loss: 0.261630, Time/step: 9.923562
2023-03-17 23:01:28,257:INFO: Epoch: 2/5, Step: 2688/2812, Lr: 0.000000067, Loss: 0.582439, Time/step: 10.165773
2023-03-17 23:09:53,297:INFO: Epoch: 2/5, Step: 2738/2812, Lr: 0.000000066, Loss: 0.351409, Time/step: 10.100795
2023-03-17 23:18:15,529:INFO: Epoch: 2/5, Step: 2788/2812, Lr: 0.000000066, Loss: 0.430110, Time/step: 10.044646
2023-03-17 23:22:19,630:INFO: Epoch 2/5 Finished, Train Loss: 0.410984
2023-03-17 23:22:21,869:INFO: Model saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1
2023-03-17 23:22:21,869:INFO: Optimizer saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.1
2023-03-17 23:24:57,719:INFO: sim matrix size: 1000, 1000
2023-03-17 23:24:57,778:INFO: 	 Length-T: 1000, Length-V:1000
2023-03-17 23:24:57,778:INFO: Text-to-Video:
2023-03-17 23:24:57,778:INFO: 	>>>  R@1: 42.2 - R@5: 70.2 - R@10: 80.9 - Median R: 2.0 - Mean R: 15.3
2023-03-17 23:24:57,779:INFO: Video-to-Text:
2023-03-17 23:24:57,779:INFO: 	>>>  V2T$R@1: 43.3 - V2T$R@5: 70.3 - V2T$R@10: 81.0 - V2T$Median R: 2.0 - V2T$Mean R: 12.4
2023-03-17 23:24:57,781:INFO: The best model is: ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1, the R1 is: 42.2000
2023-03-17 23:29:18,955:INFO: Epoch: 3/5, Step: 26/2812, Lr: 0.000000065, Loss: 0.346519, Time/step: 5.220182
2023-03-17 23:37:39,578:INFO: Epoch: 3/5, Step: 76/2812, Lr: 0.000000065, Loss: 0.277475, Time/step: 10.012452
2023-03-17 23:46:02,182:INFO: Epoch: 3/5, Step: 126/2812, Lr: 0.000000064, Loss: 0.269526, Time/step: 10.052069
2023-03-17 23:54:22,862:INFO: Epoch: 3/5, Step: 176/2812, Lr: 0.000000064, Loss: 0.223161, Time/step: 10.013593
2023-03-18 00:02:39,440:INFO: Epoch: 3/5, Step: 226/2812, Lr: 0.000000063, Loss: 0.198033, Time/step: 9.931549
2023-03-18 00:10:56,991:INFO: Epoch: 3/5, Step: 276/2812, Lr: 0.000000062, Loss: 0.115416, Time/step: 9.951015
2023-03-18 00:19:17,415:INFO: Epoch: 3/5, Step: 326/2812, Lr: 0.000000062, Loss: 0.216291, Time/step: 10.008471
2023-03-18 00:27:39,198:INFO: Epoch: 3/5, Step: 376/2812, Lr: 0.000000061, Loss: 0.142257, Time/step: 10.035652
2023-03-18 00:36:00,331:INFO: Epoch: 3/5, Step: 426/2812, Lr: 0.000000061, Loss: 0.322689, Time/step: 10.022653
2023-03-18 00:44:24,976:INFO: Epoch: 3/5, Step: 476/2812, Lr: 0.000000060, Loss: 0.300477, Time/step: 10.092907
2023-03-18 00:52:58,136:INFO: Epoch: 3/5, Step: 526/2812, Lr: 0.000000060, Loss: 0.382603, Time/step: 10.263188
2023-03-18 01:01:17,423:INFO: Epoch: 3/5, Step: 576/2812, Lr: 0.000000059, Loss: 0.391413, Time/step: 9.985733
2023-03-18 01:09:40,709:INFO: Epoch: 3/5, Step: 626/2812, Lr: 0.000000059, Loss: 0.251572, Time/step: 10.065704
2023-03-18 01:18:06,294:INFO: Epoch: 3/5, Step: 676/2812, Lr: 0.000000058, Loss: 0.175001, Time/step: 10.111691
2023-03-18 01:26:28,603:INFO: Epoch: 3/5, Step: 726/2812, Lr: 0.000000058, Loss: 0.334766, Time/step: 10.046184
2023-03-18 01:34:45,509:INFO: Epoch: 3/5, Step: 776/2812, Lr: 0.000000057, Loss: 0.169140, Time/step: 9.938105
2023-03-18 01:43:03,433:INFO: Epoch: 3/5, Step: 826/2812, Lr: 0.000000056, Loss: 0.322892, Time/step: 9.958465
2023-03-18 01:51:21,401:INFO: Epoch: 3/5, Step: 876/2812, Lr: 0.000000056, Loss: 0.253896, Time/step: 9.959360
2023-03-18 01:59:44,356:INFO: Epoch: 3/5, Step: 926/2812, Lr: 0.000000055, Loss: 0.239557, Time/step: 10.059094
2023-03-18 02:08:12,059:INFO: Epoch: 3/5, Step: 976/2812, Lr: 0.000000055, Loss: 0.214168, Time/step: 10.154049
2023-03-18 02:16:31,163:INFO: Epoch: 3/5, Step: 1026/2812, Lr: 0.000000054, Loss: 0.168951, Time/step: 9.982079
2023-03-18 02:24:53,788:INFO: Epoch: 3/5, Step: 1076/2812, Lr: 0.000000054, Loss: 0.285124, Time/step: 10.052497
2023-03-18 02:33:15,492:INFO: Epoch: 3/5, Step: 1126/2812, Lr: 0.000000053, Loss: 0.347040, Time/step: 10.034068
2023-03-18 02:41:36,588:INFO: Epoch: 3/5, Step: 1176/2812, Lr: 0.000000053, Loss: 0.242346, Time/step: 10.021910
2023-03-18 02:50:01,956:INFO: Epoch: 3/5, Step: 1226/2812, Lr: 0.000000052, Loss: 0.391397, Time/step: 10.107344
2023-03-18 02:58:22,615:INFO: Epoch: 3/5, Step: 1276/2812, Lr: 0.000000051, Loss: 0.283825, Time/step: 10.013171
2023-03-18 03:06:51,271:INFO: Epoch: 3/5, Step: 1326/2812, Lr: 0.000000051, Loss: 0.195428, Time/step: 10.173120
2023-03-18 03:15:12,339:INFO: Epoch: 3/5, Step: 1376/2812, Lr: 0.000000050, Loss: 0.223883, Time/step: 10.021340
2023-03-18 03:23:40,169:INFO: Epoch: 3/5, Step: 1426/2812, Lr: 0.000000050, Loss: 0.352100, Time/step: 10.156606
2023-03-18 03:32:04,512:INFO: Epoch: 3/5, Step: 1476/2812, Lr: 0.000000049, Loss: 0.165145, Time/step: 10.086857
2023-03-18 03:40:27,786:INFO: Epoch: 3/5, Step: 1526/2812, Lr: 0.000000049, Loss: 0.250990, Time/step: 10.065472
2023-03-18 03:48:52,923:INFO: Epoch: 3/5, Step: 1576/2812, Lr: 0.000000048, Loss: 0.322301, Time/step: 10.102725
2023-03-18 03:57:14,486:INFO: Epoch: 3/5, Step: 1626/2812, Lr: 0.000000048, Loss: 0.293019, Time/step: 10.031252
2023-03-18 04:05:42,874:INFO: Epoch: 3/5, Step: 1676/2812, Lr: 0.000000047, Loss: 0.082584, Time/step: 10.167759
2023-03-18 04:14:03,765:INFO: Epoch: 3/5, Step: 1726/2812, Lr: 0.000000046, Loss: 0.147660, Time/step: 10.017820
2023-03-18 04:22:27,845:INFO: Epoch: 3/5, Step: 1776/2812, Lr: 0.000000046, Loss: 0.159554, Time/step: 10.081586
2023-03-18 04:30:45,386:INFO: Epoch: 3/5, Step: 1826/2812, Lr: 0.000000045, Loss: 0.290010, Time/step: 9.950800
2023-03-18 04:39:05,104:INFO: Epoch: 3/5, Step: 1876/2812, Lr: 0.000000045, Loss: 0.398320, Time/step: 9.994367
2023-03-18 04:47:21,646:INFO: Epoch: 3/5, Step: 1926/2812, Lr: 0.000000044, Loss: 0.272440, Time/step: 9.930815
2023-03-18 04:55:39,313:INFO: Epoch: 3/5, Step: 1976/2812, Lr: 0.000000044, Loss: 0.182483, Time/step: 9.953346
2023-03-18 05:03:58,746:INFO: Epoch: 3/5, Step: 2026/2812, Lr: 0.000000043, Loss: 0.328137, Time/step: 9.988644
2023-03-18 05:12:21,197:INFO: Epoch: 3/5, Step: 2076/2812, Lr: 0.000000043, Loss: 0.426230, Time/step: 10.049022
2023-03-18 05:20:48,328:INFO: Epoch: 3/5, Step: 2126/2812, Lr: 0.000000042, Loss: 0.274226, Time/step: 10.142618
2023-03-18 05:29:10,782:INFO: Epoch: 3/5, Step: 2176/2812, Lr: 0.000000041, Loss: 0.289234, Time/step: 10.049057
2023-03-18 05:37:27,154:INFO: Epoch: 3/5, Step: 2226/2812, Lr: 0.000000041, Loss: 0.218349, Time/step: 9.927447
2023-03-18 05:45:49,811:INFO: Epoch: 3/5, Step: 2276/2812, Lr: 0.000000040, Loss: 0.173647, Time/step: 10.053121
2023-03-18 05:54:17,560:INFO: Epoch: 3/5, Step: 2326/2812, Lr: 0.000000040, Loss: 0.269325, Time/step: 10.154985
2023-03-18 06:02:42,463:INFO: Epoch: 3/5, Step: 2376/2812, Lr: 0.000000039, Loss: 0.250331, Time/step: 10.098038
2023-03-18 06:11:13,298:INFO: Epoch: 3/5, Step: 2426/2812, Lr: 0.000000039, Loss: 0.390920, Time/step: 10.216699
2023-03-18 06:19:33,822:INFO: Epoch: 3/5, Step: 2476/2812, Lr: 0.000000038, Loss: 0.153410, Time/step: 10.010475
2023-03-18 06:27:52,384:INFO: Epoch: 3/5, Step: 2526/2812, Lr: 0.000000038, Loss: 0.541040, Time/step: 9.971231
2023-03-18 06:36:18,273:INFO: Epoch: 3/5, Step: 2576/2812, Lr: 0.000000037, Loss: 0.229975, Time/step: 10.117765
2023-03-18 06:44:39,460:INFO: Epoch: 3/5, Step: 2626/2812, Lr: 0.000000037, Loss: 0.240749, Time/step: 10.023741
2023-03-18 06:53:01,995:INFO: Epoch: 3/5, Step: 2676/2812, Lr: 0.000000036, Loss: 0.325920, Time/step: 10.050702
2023-03-18 07:01:29,727:INFO: Epoch: 3/5, Step: 2726/2812, Lr: 0.000000035, Loss: 0.323852, Time/step: 10.154625
2023-03-18 07:09:56,685:INFO: Epoch: 3/5, Step: 2776/2812, Lr: 0.000000035, Loss: 0.345674, Time/step: 10.139163
2023-03-18 07:15:49,035:INFO: Epoch 3/5 Finished, Train Loss: 0.274886
2023-03-18 07:15:51,231:INFO: Model saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.2
2023-03-18 07:15:51,231:INFO: Optimizer saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.2
2023-03-18 07:18:26,346:INFO: sim matrix size: 1000, 1000
2023-03-18 07:18:26,406:INFO: 	 Length-T: 1000, Length-V:1000
2023-03-18 07:18:26,406:INFO: Text-to-Video:
2023-03-18 07:18:26,406:INFO: 	>>>  R@1: 41.7 - R@5: 69.5 - R@10: 80.9 - Median R: 2.0 - Mean R: 16.1
2023-03-18 07:18:26,406:INFO: Video-to-Text:
2023-03-18 07:18:26,406:INFO: 	>>>  V2T$R@1: 42.9 - V2T$R@5: 70.1 - V2T$R@10: 80.6 - V2T$Median R: 2.0 - V2T$Mean R: 11.6
2023-03-18 07:18:26,408:INFO: The best model is: ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1, the R1 is: 42.2000
2023-03-18 07:20:49,286:INFO: Epoch: 4/5, Step: 14/2812, Lr: 0.000000034, Loss: 0.261620, Time/step: 2.854257
2023-03-18 07:29:09,731:INFO: Epoch: 4/5, Step: 64/2812, Lr: 0.000000034, Loss: 0.141694, Time/step: 10.008891
2023-03-18 07:37:28,321:INFO: Epoch: 4/5, Step: 114/2812, Lr: 0.000000033, Loss: 0.302523, Time/step: 9.971777
2023-03-18 07:45:52,277:INFO: Epoch: 4/5, Step: 164/2812, Lr: 0.000000033, Loss: 0.154647, Time/step: 10.079119
2023-03-18 07:54:14,430:INFO: Epoch: 4/5, Step: 214/2812, Lr: 0.000000032, Loss: 0.276963, Time/step: 10.043064
2023-03-18 08:02:41,520:INFO: Epoch: 4/5, Step: 264/2812, Lr: 0.000000032, Loss: 0.277500, Time/step: 10.141790
2023-03-18 08:11:02,864:INFO: Epoch: 4/5, Step: 314/2812, Lr: 0.000000031, Loss: 0.176807, Time/step: 10.026871
2023-03-18 08:19:25,972:INFO: Epoch: 4/5, Step: 364/2812, Lr: 0.000000031, Loss: 0.275262, Time/step: 10.062157
2023-03-18 08:27:50,297:INFO: Epoch: 4/5, Step: 414/2812, Lr: 0.000000030, Loss: 0.164490, Time/step: 10.086473
2023-03-18 08:36:13,685:INFO: Epoch: 4/5, Step: 464/2812, Lr: 0.000000030, Loss: 0.369860, Time/step: 10.067770
2023-03-18 08:44:34,442:INFO: Epoch: 4/5, Step: 514/2812, Lr: 0.000000029, Loss: 0.257450, Time/step: 10.015124
2023-03-18 08:52:56,241:INFO: Epoch: 4/5, Step: 564/2812, Lr: 0.000000029, Loss: 0.083438, Time/step: 10.035960
2023-03-18 09:01:16,106:INFO: Epoch: 4/5, Step: 614/2812, Lr: 0.000000028, Loss: 0.192942, Time/step: 9.997299
2023-03-18 09:09:40,595:INFO: Epoch: 4/5, Step: 664/2812, Lr: 0.000000028, Loss: 0.207900, Time/step: 10.089774
2023-03-18 09:17:58,119:INFO: Epoch: 4/5, Step: 714/2812, Lr: 0.000000027, Loss: 0.148365, Time/step: 9.950481
2023-03-18 09:26:24,313:INFO: Epoch: 4/5, Step: 764/2812, Lr: 0.000000027, Loss: 0.279173, Time/step: 10.123880
2023-03-18 09:34:45,848:INFO: Epoch: 4/5, Step: 814/2812, Lr: 0.000000026, Loss: 0.188089, Time/step: 10.030674
2023-03-18 09:43:00,334:INFO: Epoch: 4/5, Step: 864/2812, Lr: 0.000000026, Loss: 0.114716, Time/step: 9.889726
2023-03-18 09:51:25,418:INFO: Epoch: 4/5, Step: 914/2812, Lr: 0.000000025, Loss: 0.271497, Time/step: 10.101660
2023-03-18 09:59:50,105:INFO: Epoch: 4/5, Step: 964/2812, Lr: 0.000000025, Loss: 0.287332, Time/step: 10.093748
2023-03-18 10:08:13,166:INFO: Epoch: 4/5, Step: 1014/2812, Lr: 0.000000024, Loss: 0.182372, Time/step: 10.061212
2023-03-18 10:16:31,411:INFO: Epoch: 4/5, Step: 1064/2812, Lr: 0.000000024, Loss: 0.214445, Time/step: 9.964879
2023-03-18 10:24:52,916:INFO: Epoch: 4/5, Step: 1114/2812, Lr: 0.000000023, Loss: 0.177269, Time/step: 10.030108
2023-03-18 10:33:22,682:INFO: Epoch: 4/5, Step: 1164/2812, Lr: 0.000000023, Loss: 0.294951, Time/step: 10.195310
2023-03-18 10:41:48,971:INFO: Epoch: 4/5, Step: 1214/2812, Lr: 0.000000022, Loss: 0.430756, Time/step: 10.125769
2023-03-18 10:50:03,233:INFO: Epoch: 4/5, Step: 1264/2812, Lr: 0.000000022, Loss: 0.192317, Time/step: 9.885229
2023-03-18 10:58:18,167:INFO: Epoch: 4/5, Step: 1314/2812, Lr: 0.000000021, Loss: 0.313688, Time/step: 9.898672
2023-03-18 11:06:43,160:INFO: Epoch: 4/5, Step: 1364/2812, Lr: 0.000000021, Loss: 0.184447, Time/step: 10.099861
2023-03-18 11:15:07,482:INFO: Epoch: 4/5, Step: 1414/2812, Lr: 0.000000021, Loss: 0.155951, Time/step: 10.086432
2023-03-18 11:23:29,156:INFO: Epoch: 4/5, Step: 1464/2812, Lr: 0.000000020, Loss: 0.292114, Time/step: 10.033467
2023-03-18 11:31:50,498:INFO: Epoch: 4/5, Step: 1514/2812, Lr: 0.000000020, Loss: 0.235273, Time/step: 10.026825
2023-03-18 11:40:18,333:INFO: Epoch: 4/5, Step: 1564/2812, Lr: 0.000000019, Loss: 0.312346, Time/step: 10.156696
2023-03-18 11:48:44,248:INFO: Epoch: 4/5, Step: 1614/2812, Lr: 0.000000019, Loss: 0.137902, Time/step: 10.118300
2023-03-18 11:57:14,088:INFO: Epoch: 4/5, Step: 1664/2812, Lr: 0.000000018, Loss: 0.092853, Time/step: 10.196799
2023-03-18 12:05:30,558:INFO: Epoch: 4/5, Step: 1714/2812, Lr: 0.000000018, Loss: 0.299319, Time/step: 9.929382
2023-03-18 12:13:58,633:INFO: Epoch: 4/5, Step: 1764/2812, Lr: 0.000000017, Loss: 0.268733, Time/step: 10.161499
2023-03-18 12:22:21,801:INFO: Epoch: 4/5, Step: 1814/2812, Lr: 0.000000017, Loss: 0.204109, Time/step: 10.063340
2023-03-18 12:30:42,364:INFO: Epoch: 4/5, Step: 1864/2812, Lr: 0.000000017, Loss: 0.095061, Time/step: 10.011246
2023-03-18 12:39:03,792:INFO: Epoch: 4/5, Step: 1914/2812, Lr: 0.000000016, Loss: 0.182306, Time/step: 10.028557
2023-03-18 12:47:26,484:INFO: Epoch: 4/5, Step: 1964/2812, Lr: 0.000000016, Loss: 0.217814, Time/step: 10.053835
2023-03-18 12:55:39,596:INFO: Epoch: 4/5, Step: 2014/2812, Lr: 0.000000015, Loss: 0.159524, Time/step: 9.862246
2023-03-18 13:04:00,634:INFO: Epoch: 4/5, Step: 2064/2812, Lr: 0.000000015, Loss: 0.304029, Time/step: 10.020751
2023-03-18 13:12:23,687:INFO: Epoch: 4/5, Step: 2114/2812, Lr: 0.000000015, Loss: 0.164345, Time/step: 10.061040
2023-03-18 13:20:47,263:INFO: Epoch: 4/5, Step: 2164/2812, Lr: 0.000000014, Loss: 0.222610, Time/step: 10.071524
2023-03-18 13:29:08,052:INFO: Epoch: 4/5, Step: 2214/2812, Lr: 0.000000014, Loss: 0.131677, Time/step: 10.015777
2023-03-18 13:37:24,739:INFO: Epoch: 4/5, Step: 2264/2812, Lr: 0.000000013, Loss: 0.132627, Time/step: 9.933720
2023-03-18 13:45:48,040:INFO: Epoch: 4/5, Step: 2314/2812, Lr: 0.000000013, Loss: 0.455019, Time/step: 10.066018
2023-03-18 13:54:07,605:INFO: Epoch: 4/5, Step: 2364/2812, Lr: 0.000000013, Loss: 0.153965, Time/step: 9.991293
2023-03-18 14:02:30,389:INFO: Epoch: 4/5, Step: 2414/2812, Lr: 0.000000012, Loss: 0.245666, Time/step: 10.055680
2023-03-18 14:10:51,040:INFO: Epoch: 4/5, Step: 2464/2812, Lr: 0.000000012, Loss: 0.098452, Time/step: 10.013021
2023-03-18 14:19:17,123:INFO: Epoch: 4/5, Step: 2514/2812, Lr: 0.000000012, Loss: 0.129564, Time/step: 10.121648
2023-03-18 14:27:41,550:INFO: Epoch: 4/5, Step: 2564/2812, Lr: 0.000000011, Loss: 0.393976, Time/step: 10.088537
2023-03-18 14:36:02,414:INFO: Epoch: 4/5, Step: 2614/2812, Lr: 0.000000011, Loss: 0.290691, Time/step: 10.017270
2023-03-18 14:44:20,407:INFO: Epoch: 4/5, Step: 2664/2812, Lr: 0.000000011, Loss: 0.212376, Time/step: 9.959863
2023-03-18 14:52:44,427:INFO: Epoch: 4/5, Step: 2714/2812, Lr: 0.000000010, Loss: 0.197225, Time/step: 10.080380
2023-03-18 15:01:07,493:INFO: Epoch: 4/5, Step: 2764/2812, Lr: 0.000000010, Loss: 0.334101, Time/step: 10.061320
2023-03-18 15:09:00,293:INFO: Epoch 4/5 Finished, Train Loss: 0.212898
2023-03-18 15:09:02,540:INFO: Model saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3
2023-03-18 15:09:02,540:INFO: Optimizer saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.3
2023-03-18 15:11:38,194:INFO: sim matrix size: 1000, 1000
2023-03-18 15:11:38,253:INFO: 	 Length-T: 1000, Length-V:1000
2023-03-18 15:11:38,253:INFO: Text-to-Video:
2023-03-18 15:11:38,253:INFO: 	>>>  R@1: 41.6 - R@5: 69.0 - R@10: 79.6 - Median R: 2.0 - Mean R: 16.6
2023-03-18 15:11:38,254:INFO: Video-to-Text:
2023-03-18 15:11:38,254:INFO: 	>>>  V2T$R@1: 41.2 - V2T$R@5: 69.7 - V2T$R@10: 80.4 - V2T$Median R: 2.0 - V2T$Mean R: 12.1
2023-03-18 15:11:38,254:INFO: The best model is: ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1, the R1 is: 42.2000
2023-03-18 15:11:58,663:INFO: Epoch: 5/5, Step: 2/2812, Lr: 0.000000010, Loss: 0.097534, Time/step: 0.404883
2023-03-18 15:20:15,815:INFO: Epoch: 5/5, Step: 52/2812, Lr: 0.000000009, Loss: 0.132559, Time/step: 9.943033
2023-03-18 15:28:38,539:INFO: Epoch: 5/5, Step: 102/2812, Lr: 0.000000009, Loss: 0.213117, Time/step: 10.054473
2023-03-18 15:37:01,816:INFO: Epoch: 5/5, Step: 152/2812, Lr: 0.000000009, Loss: 0.158781, Time/step: 10.065533
2023-03-18 15:45:26,351:INFO: Epoch: 5/5, Step: 202/2812, Lr: 0.000000008, Loss: 0.233258, Time/step: 10.090695
2023-03-18 15:53:46,102:INFO: Epoch: 5/5, Step: 252/2812, Lr: 0.000000008, Loss: 0.103721, Time/step: 9.995009
2023-03-18 16:02:09,277:INFO: Epoch: 5/5, Step: 302/2812, Lr: 0.000000008, Loss: 0.170011, Time/step: 10.063502
2023-03-18 16:10:24,022:INFO: Epoch: 5/5, Step: 352/2812, Lr: 0.000000007, Loss: 0.194614, Time/step: 9.894889
2023-03-18 16:18:48,779:INFO: Epoch: 5/5, Step: 402/2812, Lr: 0.000000007, Loss: 0.124949, Time/step: 10.095127
2023-03-18 16:27:05,665:INFO: Epoch: 5/5, Step: 452/2812, Lr: 0.000000007, Loss: 0.231190, Time/step: 9.937728
2023-03-18 16:35:35,774:INFO: Epoch: 5/5, Step: 502/2812, Lr: 0.000000007, Loss: 0.234656, Time/step: 10.202171
2023-03-18 16:43:52,871:INFO: Epoch: 5/5, Step: 552/2812, Lr: 0.000000006, Loss: 0.120781, Time/step: 9.941932
2023-03-18 16:52:09,599:INFO: Epoch: 5/5, Step: 602/2812, Lr: 0.000000006, Loss: 0.227931, Time/step: 9.934553
2023-03-18 17:00:36,133:INFO: Epoch: 5/5, Step: 652/2812, Lr: 0.000000006, Loss: 0.160699, Time/step: 10.130669
2023-03-18 17:08:59,415:INFO: Epoch: 5/5, Step: 702/2812, Lr: 0.000000005, Loss: 0.236524, Time/step: 10.065637
2023-03-18 17:17:14,946:INFO: Epoch: 5/5, Step: 752/2812, Lr: 0.000000005, Loss: 0.223314, Time/step: 9.910603
2023-03-18 17:25:36,433:INFO: Epoch: 5/5, Step: 802/2812, Lr: 0.000000005, Loss: 0.120631, Time/step: 10.029727
2023-03-18 17:33:55,024:INFO: Epoch: 5/5, Step: 852/2812, Lr: 0.000000005, Loss: 0.077997, Time/step: 9.971811
2023-03-18 17:42:17,655:INFO: Epoch: 5/5, Step: 902/2812, Lr: 0.000000004, Loss: 0.318378, Time/step: 10.052612
2023-03-18 17:50:41,529:INFO: Epoch: 5/5, Step: 952/2812, Lr: 0.000000004, Loss: 0.159536, Time/step: 10.077476
2023-03-18 17:59:06,752:INFO: Epoch: 5/5, Step: 1002/2812, Lr: 0.000000004, Loss: 0.210665, Time/step: 10.104462
2023-03-18 18:07:28,285:INFO: Epoch: 5/5, Step: 1052/2812, Lr: 0.000000004, Loss: 0.260208, Time/step: 10.030656
2023-03-18 18:15:53,933:INFO: Epoch: 5/5, Step: 1102/2812, Lr: 0.000000004, Loss: 0.214004, Time/step: 10.112962
2023-03-18 18:24:21,467:INFO: Epoch: 5/5, Step: 1152/2812, Lr: 0.000000003, Loss: 0.227580, Time/step: 10.150667
2023-03-18 18:32:45,726:INFO: Epoch: 5/5, Step: 1202/2812, Lr: 0.000000003, Loss: 0.122098, Time/step: 10.085171
2023-03-18 18:41:06,843:INFO: Epoch: 5/5, Step: 1252/2812, Lr: 0.000000003, Loss: 0.242130, Time/step: 10.022336
2023-03-18 18:49:29,749:INFO: Epoch: 5/5, Step: 1302/2812, Lr: 0.000000003, Loss: 0.146917, Time/step: 10.058115
2023-03-18 18:57:51,985:INFO: Epoch: 5/5, Step: 1352/2812, Lr: 0.000000003, Loss: 0.126018, Time/step: 10.044699
2023-03-18 19:06:12,202:INFO: Epoch: 5/5, Step: 1402/2812, Lr: 0.000000002, Loss: 0.266081, Time/step: 10.004340
2023-03-18 19:14:29,228:INFO: Epoch: 5/5, Step: 1452/2812, Lr: 0.000000002, Loss: 0.197009, Time/step: 9.940515
2023-03-18 19:22:50,625:INFO: Epoch: 5/5, Step: 1502/2812, Lr: 0.000000002, Loss: 0.137175, Time/step: 10.027928
2023-03-18 19:31:12,290:INFO: Epoch: 5/5, Step: 1552/2812, Lr: 0.000000002, Loss: 0.178692, Time/step: 10.033302
2023-03-18 19:39:35,537:INFO: Epoch: 5/5, Step: 1602/2812, Lr: 0.000000002, Loss: 0.266879, Time/step: 10.064931
2023-03-18 19:47:59,382:INFO: Epoch: 5/5, Step: 1652/2812, Lr: 0.000000002, Loss: 0.123536, Time/step: 10.076894
2023-03-18 19:56:22,545:INFO: Epoch: 5/5, Step: 1702/2812, Lr: 0.000000002, Loss: 0.180658, Time/step: 10.063246
2023-03-18 20:04:48,388:INFO: Epoch: 5/5, Step: 1752/2812, Lr: 0.000000001, Loss: 0.228591, Time/step: 10.116856
2023-03-18 20:13:07,686:INFO: Epoch: 5/5, Step: 1802/2812, Lr: 0.000000001, Loss: 0.181344, Time/step: 9.985954
2023-03-18 20:21:27,688:INFO: Epoch: 5/5, Step: 1852/2812, Lr: 0.000000001, Loss: 0.181620, Time/step: 10.000040
2023-03-18 20:29:50,133:INFO: Epoch: 5/5, Step: 1902/2812, Lr: 0.000000001, Loss: 0.187075, Time/step: 10.048883
2023-03-18 20:38:19,889:INFO: Epoch: 5/5, Step: 1952/2812, Lr: 0.000000001, Loss: 0.196668, Time/step: 10.195103
2023-03-18 20:46:43,944:INFO: Epoch: 5/5, Step: 2002/2812, Lr: 0.000000001, Loss: 0.141200, Time/step: 10.081096
2023-03-18 20:55:12,854:INFO: Epoch: 5/5, Step: 2052/2812, Lr: 0.000000001, Loss: 0.210644, Time/step: 10.178188
2023-03-18 21:03:34,322:INFO: Epoch: 5/5, Step: 2102/2812, Lr: 0.000000001, Loss: 0.401365, Time/step: 10.029356
2023-03-18 21:11:59,658:INFO: Epoch: 5/5, Step: 2152/2812, Lr: 0.000000001, Loss: 0.244556, Time/step: 10.106714
2023-03-18 21:20:20,337:INFO: Epoch: 5/5, Step: 2202/2812, Lr: 0.000000000, Loss: 0.231337, Time/step: 10.013583
2023-03-18 21:28:44,511:INFO: Epoch: 5/5, Step: 2252/2812, Lr: 0.000000000, Loss: 0.140907, Time/step: 10.083468
2023-03-18 21:37:03,136:INFO: Epoch: 5/5, Step: 2302/2812, Lr: 0.000000000, Loss: 0.306771, Time/step: 9.972499
2023-03-18 21:45:26,254:INFO: Epoch: 5/5, Step: 2352/2812, Lr: 0.000000000, Loss: 0.193185, Time/step: 10.062344
2023-03-18 21:53:52,675:INFO: Epoch: 5/5, Step: 2402/2812, Lr: 0.000000000, Loss: 0.116426, Time/step: 10.128420
2023-03-18 22:02:15,956:INFO: Epoch: 5/5, Step: 2452/2812, Lr: 0.000000000, Loss: 0.256941, Time/step: 10.065617
2023-03-18 22:10:38,166:INFO: Epoch: 5/5, Step: 2502/2812, Lr: 0.000000000, Loss: 0.245691, Time/step: 10.044189
2023-03-18 22:19:04,062:INFO: Epoch: 5/5, Step: 2552/2812, Lr: 0.000000000, Loss: 0.324943, Time/step: 10.117928
2023-03-18 22:27:29,367:INFO: Epoch: 5/5, Step: 2602/2812, Lr: 0.000000000, Loss: 0.300304, Time/step: 10.106071
2023-03-18 22:35:46,464:INFO: Epoch: 5/5, Step: 2652/2812, Lr: 0.000000000, Loss: 0.314414, Time/step: 9.941943
2023-03-18 22:44:13,709:INFO: Epoch: 5/5, Step: 2702/2812, Lr: 0.000000000, Loss: 0.158100, Time/step: 10.144895
2023-03-18 22:52:38,606:INFO: Epoch: 5/5, Step: 2752/2812, Lr: 0.000000000, Loss: 0.165205, Time/step: 10.097924
2023-03-18 23:00:59,786:INFO: Epoch: 5/5, Step: 2802/2812, Lr: 0.000000000, Loss: 0.219178, Time/step: 10.023599
2023-03-18 23:02:40,745:INFO: Epoch 5/5 Finished, Train Loss: 0.194010
2023-03-18 23:02:42,957:INFO: Model saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2023-03-18 23:02:42,958:INFO: Optimizer saved to ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.4
2023-03-18 23:05:18,548:INFO: sim matrix size: 1000, 1000
2023-03-18 23:05:18,607:INFO: 	 Length-T: 1000, Length-V:1000
2023-03-18 23:05:18,607:INFO: Text-to-Video:
2023-03-18 23:05:18,607:INFO: 	>>>  R@1: 41.7 - R@5: 68.7 - R@10: 79.6 - Median R: 2.0 - Mean R: 16.6
2023-03-18 23:05:18,607:INFO: Video-to-Text:
2023-03-18 23:05:18,607:INFO: 	>>>  V2T$R@1: 41.8 - V2T$R@5: 69.5 - V2T$R@10: 80.4 - V2T$Median R: 2.0 - V2T$Mean R: 12.1
2023-03-18 23:05:18,608:INFO: The best model is: ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1, the R1 is: 42.2000
