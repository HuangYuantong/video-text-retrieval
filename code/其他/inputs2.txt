2023-05-22 23:28:41,688:INFO: 

======================================
Effective parameters:
2023-05-22 23:28:41,688:INFO:   <<< add_adapter_to_vit: False
2023-05-22 23:28:41,688:INFO:   <<< batch_size: 32
2023-05-22 23:28:41,688:INFO:   <<< coef_lr: 0.001
2023-05-22 23:28:41,688:INFO:   <<< do_train: True
2023-05-22 23:28:41,688:INFO:   <<< froze_model: False
2023-05-22 23:28:41,688:INFO:   <<< load_Clip4Clip: False
2023-05-22 23:28:41,689:INFO:   <<< lr: 0.0001
2023-05-22 23:28:41,689:INFO:   <<< output_dir: /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform
2023-05-22 23:28:41,689:INFO:   <<< rank: 0
2023-05-22 23:28:41,689:INFO:   <<< shuffle: False
2023-05-22 23:28:41,689:INFO:   <<< world_size: 1
2023-05-22 23:28:41,689:INFO: device: cuda:0 n_gpu: 1
2023-05-22 23:28:44,707:INFO: 是否在视频编码器里添加Adapter：False
2023-05-22 23:28:44,707:INFO: 是否在文字编码器里添加Adapter：False
2023-05-22 23:28:44,708:INFO: => loaded successfully from: "/media/dc/新加卷/dc/hyt/hyt2/res/ViT-B-32.pt"
2023-05-22 23:28:44,708:INFO: 无冻结参数，总参数量：151.2773M（151277313）
2023-05-22 23:28:44,713:INFO: ***** Running test *****
2023-05-22 23:28:44,713:INFO:   Num examples = 1000
2023-05-22 23:28:44,713:INFO:   Batch size = 32
2023-05-22 23:28:44,713:INFO:   Num steps = 32
2023-05-22 23:28:44,713:INFO: ***** Running val *****
2023-05-22 23:28:44,713:INFO:   Num examples = 1000
2023-05-22 23:28:44,809:INFO: ***** Running training *****
2023-05-22 23:28:44,809:INFO:   Num examples = 180000
2023-05-22 23:28:44,809:INFO:   Batch size = 32
2023-05-22 23:28:44,810:INFO:   Num steps = 28125
2023-05-22 23:42:11,130:INFO: Epoch: 1/5, Step: 100/5625, Lr: 0.000000004, Loss: 0.938334, Time/step: 8.063197
2023-05-22 23:56:10,676:INFO: Epoch: 1/5, Step: 200/5625, Lr: 0.000000007, Loss: 0.587694, Time/step: 8.395212
2023-05-23 00:06:00,469:INFO: Epoch: 1/5, Step: 300/5625, Lr: 0.000000011, Loss: 0.612280, Time/step: 5.897930
2023-05-23 00:12:51,087:INFO: Epoch: 1/5, Step: 400/5625, Lr: 0.000000014, Loss: 0.487767, Time/step: 4.105945
2023-05-23 00:17:45,395:INFO: Epoch: 1/5, Step: 500/5625, Lr: 0.000000018, Loss: 0.695553, Time/step: 2.942629
2023-05-23 00:21:23,731:INFO: Epoch: 1/5, Step: 600/5625, Lr: 0.000000021, Loss: 0.423261, Time/step: 2.182526
2023-05-23 00:23:48,248:INFO: Epoch: 1/5, Step: 700/5625, Lr: 0.000000025, Loss: 0.528999, Time/step: 1.445063
2023-05-23 00:25:26,731:INFO: Epoch: 1/5, Step: 800/5625, Lr: 0.000000028, Loss: 0.435629, Time/step: 0.984835
2023-05-23 00:26:46,013:INFO: Epoch: 1/5, Step: 900/5625, Lr: 0.000000032, Loss: 0.700822, Time/step: 0.792814
2023-05-23 00:27:59,244:INFO: Epoch: 1/5, Step: 1000/5625, Lr: 0.000000036, Loss: 0.572881, Time/step: 0.732303
2023-05-23 00:28:54,289:INFO: Epoch: 1/5, Step: 1100/5625, Lr: 0.000000039, Loss: 0.545098, Time/step: 0.550273
2023-05-23 00:29:39,042:INFO: Epoch: 1/5, Step: 1200/5625, Lr: 0.000000043, Loss: 0.479105, Time/step: 0.447523
2023-05-23 00:30:21,466:INFO: Epoch: 1/5, Step: 1300/5625, Lr: 0.000000046, Loss: 0.741584, Time/step: 0.424182
2023-05-23 00:31:03,286:INFO: Epoch: 1/5, Step: 1400/5625, Lr: 0.000000050, Loss: 0.657366, Time/step: 0.418192
2023-05-23 00:31:44,614:INFO: Epoch: 1/5, Step: 1500/5625, Lr: 0.000000053, Loss: 0.870354, Time/step: 0.413276
2023-05-23 00:32:25,578:INFO: Epoch: 1/5, Step: 1600/5625, Lr: 0.000000057, Loss: 1.129760, Time/step: 0.409640
2023-05-23 00:33:06,164:INFO: Epoch: 1/5, Step: 1700/5625, Lr: 0.000000060, Loss: 0.383264, Time/step: 0.405854
2023-05-23 00:33:46,693:INFO: Epoch: 1/5, Step: 1800/5625, Lr: 0.000000064, Loss: 0.799318, Time/step: 0.405292
2023-05-23 00:34:27,264:INFO: Epoch: 1/5, Step: 1900/5625, Lr: 0.000000068, Loss: 0.379670, Time/step: 0.405704
2023-05-23 00:35:07,829:INFO: Epoch: 1/5, Step: 2000/5625, Lr: 0.000000071, Loss: 0.321594, Time/step: 0.405643
2023-05-23 00:35:48,405:INFO: Epoch: 1/5, Step: 2100/5625, Lr: 0.000000075, Loss: 0.273642, Time/step: 0.405758
2023-05-23 00:36:28,985:INFO: Epoch: 1/5, Step: 2200/5625, Lr: 0.000000078, Loss: 0.308852, Time/step: 0.405801
2023-05-23 00:37:09,574:INFO: Epoch: 1/5, Step: 2300/5625, Lr: 0.000000082, Loss: 0.633376, Time/step: 0.405876
2023-05-23 00:37:50,167:INFO: Epoch: 1/5, Step: 2400/5625, Lr: 0.000000085, Loss: 0.593314, Time/step: 0.405934
2023-05-23 00:38:30,760:INFO: Epoch: 1/5, Step: 2500/5625, Lr: 0.000000089, Loss: 0.486448, Time/step: 0.405924
2023-05-23 00:39:11,355:INFO: Epoch: 1/5, Step: 2600/5625, Lr: 0.000000092, Loss: 0.437000, Time/step: 0.405942
2023-05-23 00:39:51,958:INFO: Epoch: 1/5, Step: 2700/5625, Lr: 0.000000096, Loss: 0.721977, Time/step: 0.406029
2023-05-23 00:40:32,563:INFO: Epoch: 1/5, Step: 2800/5625, Lr: 0.000000100, Loss: 0.529822, Time/step: 0.406051
2023-05-23 00:41:13,176:INFO: Epoch: 1/5, Step: 2900/5625, Lr: 0.000000097, Loss: 0.679897, Time/step: 0.406120
2023-05-23 00:41:53,786:INFO: Epoch: 1/5, Step: 3000/5625, Lr: 0.000000097, Loss: 0.297552, Time/step: 0.406101
2023-05-23 00:42:34,410:INFO: Epoch: 1/5, Step: 3100/5625, Lr: 0.000000097, Loss: 0.871619, Time/step: 0.406232
2023-05-23 00:43:15,014:INFO: Epoch: 1/5, Step: 3200/5625, Lr: 0.000000097, Loss: 0.492225, Time/step: 0.406035
2023-05-23 00:43:55,627:INFO: Epoch: 1/5, Step: 3300/5625, Lr: 0.000000097, Loss: 0.453389, Time/step: 0.406122
2023-05-23 00:44:36,247:INFO: Epoch: 1/5, Step: 3400/5625, Lr: 0.000000096, Loss: 0.083692, Time/step: 0.406202
2023-05-23 00:45:16,909:INFO: Epoch: 1/5, Step: 3500/5625, Lr: 0.000000096, Loss: 0.447573, Time/step: 0.406617
2023-05-23 00:45:57,627:INFO: Epoch: 1/5, Step: 3600/5625, Lr: 0.000000096, Loss: 0.420958, Time/step: 0.407177
2023-05-23 00:46:38,341:INFO: Epoch: 1/5, Step: 3700/5625, Lr: 0.000000096, Loss: 0.505166, Time/step: 0.407130
2023-05-23 00:47:19,049:INFO: Epoch: 1/5, Step: 3800/5625, Lr: 0.000000096, Loss: 0.709710, Time/step: 0.407072
2023-05-23 00:47:59,754:INFO: Epoch: 1/5, Step: 3900/5625, Lr: 0.000000095, Loss: 0.232145, Time/step: 0.407055
2023-05-23 00:48:40,462:INFO: Epoch: 1/5, Step: 4000/5625, Lr: 0.000000095, Loss: 0.779137, Time/step: 0.407074
2023-05-23 00:49:21,178:INFO: Epoch: 1/5, Step: 4100/5625, Lr: 0.000000095, Loss: 0.380652, Time/step: 0.407150
2023-05-23 00:50:01,915:INFO: Epoch: 1/5, Step: 4200/5625, Lr: 0.000000095, Loss: 0.460170, Time/step: 0.407367
2023-05-23 00:50:42,626:INFO: Epoch: 1/5, Step: 4300/5625, Lr: 0.000000094, Loss: 0.130729, Time/step: 0.407107
2023-05-23 00:51:23,345:INFO: Epoch: 1/5, Step: 4400/5625, Lr: 0.000000094, Loss: 0.442656, Time/step: 0.407189
2023-05-23 00:52:04,073:INFO: Epoch: 1/5, Step: 4500/5625, Lr: 0.000000094, Loss: 0.336294, Time/step: 0.407271
2023-05-23 00:52:44,805:INFO: Epoch: 1/5, Step: 4600/5625, Lr: 0.000000094, Loss: 0.198024, Time/step: 0.407314
2023-05-23 00:53:25,534:INFO: Epoch: 1/5, Step: 4700/5625, Lr: 0.000000093, Loss: 0.468051, Time/step: 0.407292
2023-05-23 00:54:06,270:INFO: Epoch: 1/5, Step: 4800/5625, Lr: 0.000000093, Loss: 0.477330, Time/step: 0.407353
2023-05-23 00:54:47,000:INFO: Epoch: 1/5, Step: 4900/5625, Lr: 0.000000093, Loss: 0.349974, Time/step: 0.407298
2023-05-23 00:55:27,747:INFO: Epoch: 1/5, Step: 5000/5625, Lr: 0.000000092, Loss: 0.468407, Time/step: 0.407463
2023-05-23 00:56:08,498:INFO: Epoch: 1/5, Step: 5100/5625, Lr: 0.000000092, Loss: 0.405914, Time/step: 0.407513
2023-05-23 00:56:49,267:INFO: Epoch: 1/5, Step: 5200/5625, Lr: 0.000000092, Loss: 0.489334, Time/step: 0.407686
2023-05-23 00:57:30,006:INFO: Epoch: 1/5, Step: 5300/5625, Lr: 0.000000091, Loss: 0.148881, Time/step: 0.407386
2023-05-23 00:58:10,748:INFO: Epoch: 1/5, Step: 5400/5625, Lr: 0.000000091, Loss: 0.429060, Time/step: 0.407411
2023-05-23 00:58:51,498:INFO: Epoch: 1/5, Step: 5500/5625, Lr: 0.000000091, Loss: 0.193282, Time/step: 0.407494
2023-05-23 00:59:32,231:INFO: Epoch: 1/5, Step: 5600/5625, Lr: 0.000000091, Loss: 0.745635, Time/step: 0.407326
2023-05-23 00:59:42,471:INFO: Epoch 1/5 Finished, Train Loss: 0.509567
2023-05-23 00:59:45,113:INFO: Model saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.0
2023-05-23 00:59:45,113:INFO: Optimizer saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_opt.bin.0
2023-05-23 00:59:45,113:INFO: Eval on val dataset
2023-05-23 00:59:54,311:INFO: sim matrix size: 1000, 1000
2023-05-23 00:59:54,372:INFO: 	 Length-T: 1000, Length-V:1000
2023-05-23 00:59:54,372:INFO: Text-to-Video:
2023-05-23 00:59:54,372:INFO: 	>>>  R@1: 39.8 - R@5: 68.2 - R@10: 78.2 - Median R: 2.0 - Mean R: 16.6
2023-05-23 00:59:54,373:INFO: Video-to-Text:
2023-05-23 00:59:54,373:INFO: 	>>>  V2T$R@1: 41.6 - V2T$R@5: 67.4 - V2T$R@10: 79.0 - V2T$Median R: 2.0 - V2T$Mean R: 12.0
2023-05-23 00:59:54,373:INFO: The best model is: /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.0, the R1 is: 39.8000
2023-05-23 01:00:25,066:INFO: Epoch: 2/5, Step: 75/5625, Lr: 0.000000090, Loss: 0.147121, Time/step: 0.305709
2023-05-23 01:01:05,356:INFO: Epoch: 2/5, Step: 175/5625, Lr: 0.000000090, Loss: 0.202870, Time/step: 0.402896
2023-05-23 01:01:45,705:INFO: Epoch: 2/5, Step: 275/5625, Lr: 0.000000090, Loss: 0.401497, Time/step: 0.403488
2023-05-23 01:02:26,054:INFO: Epoch: 2/5, Step: 375/5625, Lr: 0.000000089, Loss: 0.266179, Time/step: 0.403481
2023-05-23 01:03:06,421:INFO: Epoch: 2/5, Step: 475/5625, Lr: 0.000000089, Loss: 0.073411, Time/step: 0.403667
2023-05-23 01:03:46,763:INFO: Epoch: 2/5, Step: 575/5625, Lr: 0.000000088, Loss: 0.076241, Time/step: 0.403422
2023-05-23 01:04:27,074:INFO: Epoch: 2/5, Step: 675/5625, Lr: 0.000000088, Loss: 0.423620, Time/step: 0.403101
2023-05-23 01:05:07,433:INFO: Epoch: 2/5, Step: 775/5625, Lr: 0.000000088, Loss: 0.057579, Time/step: 0.403583
2023-05-23 01:05:47,734:INFO: Epoch: 2/5, Step: 875/5625, Lr: 0.000000087, Loss: 0.078823, Time/step: 0.403012
2023-05-23 01:06:28,026:INFO: Epoch: 2/5, Step: 975/5625, Lr: 0.000000087, Loss: 0.231252, Time/step: 0.402915
2023-05-23 01:07:08,403:INFO: Epoch: 2/5, Step: 1075/5625, Lr: 0.000000087, Loss: 0.328340, Time/step: 0.403768
2023-05-23 01:07:48,756:INFO: Epoch: 2/5, Step: 1175/5625, Lr: 0.000000086, Loss: 0.225224, Time/step: 0.403522
2023-05-23 01:08:29,071:INFO: Epoch: 2/5, Step: 1275/5625, Lr: 0.000000086, Loss: 0.556440, Time/step: 0.403145
2023-05-23 01:09:09,430:INFO: Epoch: 2/5, Step: 1375/5625, Lr: 0.000000085, Loss: 0.468293, Time/step: 0.403592
2023-05-23 01:09:49,815:INFO: Epoch: 2/5, Step: 1475/5625, Lr: 0.000000085, Loss: 0.359420, Time/step: 0.403844
2023-05-23 01:10:30,198:INFO: Epoch: 2/5, Step: 1575/5625, Lr: 0.000000085, Loss: 0.359080, Time/step: 0.403824
2023-05-23 01:11:10,610:INFO: Epoch: 2/5, Step: 1675/5625, Lr: 0.000000084, Loss: 0.277623, Time/step: 0.404112
2023-05-23 01:11:51,011:INFO: Epoch: 2/5, Step: 1775/5625, Lr: 0.000000084, Loss: 0.542679, Time/step: 0.404008
2023-05-23 01:12:31,410:INFO: Epoch: 2/5, Step: 1875/5625, Lr: 0.000000083, Loss: 0.287776, Time/step: 0.403987
2023-05-23 01:13:11,779:INFO: Epoch: 2/5, Step: 1975/5625, Lr: 0.000000083, Loss: 0.378980, Time/step: 0.403689
2023-05-23 01:13:52,173:INFO: Epoch: 2/5, Step: 2075/5625, Lr: 0.000000083, Loss: 0.122738, Time/step: 0.403938
2023-05-23 01:14:32,571:INFO: Epoch: 2/5, Step: 2175/5625, Lr: 0.000000082, Loss: 0.125139, Time/step: 0.403977
2023-05-23 01:15:12,927:INFO: Epoch: 2/5, Step: 2275/5625, Lr: 0.000000082, Loss: 0.358232, Time/step: 0.403549
2023-05-23 01:15:53,312:INFO: Epoch: 2/5, Step: 2375/5625, Lr: 0.000000081, Loss: 0.172263, Time/step: 0.403847
2023-05-23 01:16:33,692:INFO: Epoch: 2/5, Step: 2475/5625, Lr: 0.000000081, Loss: 0.246184, Time/step: 0.403792
2023-05-23 01:17:14,016:INFO: Epoch: 2/5, Step: 2575/5625, Lr: 0.000000080, Loss: 0.124412, Time/step: 0.403237
2023-05-23 01:17:54,400:INFO: Epoch: 2/5, Step: 2675/5625, Lr: 0.000000080, Loss: 0.411587, Time/step: 0.403843
2023-05-23 01:18:34,807:INFO: Epoch: 2/5, Step: 2775/5625, Lr: 0.000000080, Loss: 0.459983, Time/step: 0.404065
2023-05-23 01:19:15,200:INFO: Epoch: 2/5, Step: 2875/5625, Lr: 0.000000079, Loss: 0.189346, Time/step: 0.403925
2023-05-23 01:19:55,602:INFO: Epoch: 2/5, Step: 2975/5625, Lr: 0.000000079, Loss: 0.328632, Time/step: 0.404014
2023-05-23 01:20:36,011:INFO: Epoch: 2/5, Step: 3075/5625, Lr: 0.000000078, Loss: 0.459450, Time/step: 0.404085
2023-05-23 01:21:16,423:INFO: Epoch: 2/5, Step: 3175/5625, Lr: 0.000000078, Loss: 0.295098, Time/step: 0.404113
2023-05-23 01:21:56,835:INFO: Epoch: 2/5, Step: 3275/5625, Lr: 0.000000077, Loss: 0.186230, Time/step: 0.404119
2023-05-23 01:22:37,268:INFO: Epoch: 2/5, Step: 3375/5625, Lr: 0.000000077, Loss: 0.054565, Time/step: 0.404324
2023-05-23 01:23:17,680:INFO: Epoch: 2/5, Step: 3475/5625, Lr: 0.000000076, Loss: 0.367568, Time/step: 0.404118
2023-05-23 01:23:58,086:INFO: Epoch: 2/5, Step: 3575/5625, Lr: 0.000000076, Loss: 0.292960, Time/step: 0.404062
2023-05-23 01:24:38,486:INFO: Epoch: 2/5, Step: 3675/5625, Lr: 0.000000075, Loss: 0.045644, Time/step: 0.403989
2023-05-23 01:25:18,891:INFO: Epoch: 2/5, Step: 3775/5625, Lr: 0.000000075, Loss: 0.157635, Time/step: 0.404048
2023-05-23 01:25:59,313:INFO: Epoch: 2/5, Step: 3875/5625, Lr: 0.000000074, Loss: 0.117731, Time/step: 0.404220
2023-05-23 01:26:39,762:INFO: Epoch: 2/5, Step: 3975/5625, Lr: 0.000000074, Loss: 0.287283, Time/step: 0.404478
2023-05-23 01:27:20,167:INFO: Epoch: 2/5, Step: 4075/5625, Lr: 0.000000073, Loss: 0.158702, Time/step: 0.404046
2023-05-23 01:28:00,577:INFO: Epoch: 2/5, Step: 4175/5625, Lr: 0.000000073, Loss: 0.381761, Time/step: 0.404100
2023-05-23 01:28:40,972:INFO: Epoch: 2/5, Step: 4275/5625, Lr: 0.000000072, Loss: 0.254568, Time/step: 0.403946
2023-05-23 01:29:21,390:INFO: Epoch: 2/5, Step: 4375/5625, Lr: 0.000000072, Loss: 0.101525, Time/step: 0.404179
2023-05-23 01:30:01,807:INFO: Epoch: 2/5, Step: 4475/5625, Lr: 0.000000071, Loss: 0.208853, Time/step: 0.404164
2023-05-23 01:30:42,228:INFO: Epoch: 2/5, Step: 4575/5625, Lr: 0.000000071, Loss: 0.359423, Time/step: 0.404202
2023-05-23 01:31:22,639:INFO: Epoch: 2/5, Step: 4675/5625, Lr: 0.000000070, Loss: 0.260659, Time/step: 0.404109
2023-05-23 01:32:03,084:INFO: Epoch: 2/5, Step: 4775/5625, Lr: 0.000000070, Loss: 0.135489, Time/step: 0.404447
2023-05-23 01:32:43,535:INFO: Epoch: 2/5, Step: 4875/5625, Lr: 0.000000069, Loss: 0.212160, Time/step: 0.404505
2023-05-23 01:33:23,999:INFO: Epoch: 2/5, Step: 4975/5625, Lr: 0.000000069, Loss: 0.252610, Time/step: 0.404638
2023-05-23 01:34:04,453:INFO: Epoch: 2/5, Step: 5075/5625, Lr: 0.000000068, Loss: 0.351977, Time/step: 0.404537
2023-05-23 01:34:44,896:INFO: Epoch: 2/5, Step: 5175/5625, Lr: 0.000000068, Loss: 0.077566, Time/step: 0.404430
2023-05-23 01:35:25,352:INFO: Epoch: 2/5, Step: 5275/5625, Lr: 0.000000067, Loss: 0.703707, Time/step: 0.404556
2023-05-23 01:36:05,812:INFO: Epoch: 2/5, Step: 5375/5625, Lr: 0.000000067, Loss: 0.404185, Time/step: 0.404592
2023-05-23 01:36:46,260:INFO: Epoch: 2/5, Step: 5475/5625, Lr: 0.000000066, Loss: 0.246970, Time/step: 0.404479
2023-05-23 01:37:26,685:INFO: Epoch: 2/5, Step: 5575/5625, Lr: 0.000000066, Loss: 0.237958, Time/step: 0.404247
2023-05-23 01:37:46,975:INFO: Epoch 2/5 Finished, Train Loss: 0.285973
2023-05-23 01:37:49,777:INFO: Model saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.1
2023-05-23 01:37:49,777:INFO: Optimizer saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_opt.bin.1
2023-05-23 01:37:49,777:INFO: Eval on val dataset
2023-05-23 01:37:59,002:INFO: sim matrix size: 1000, 1000
2023-05-23 01:37:59,064:INFO: 	 Length-T: 1000, Length-V:1000
2023-05-23 01:37:59,064:INFO: Text-to-Video:
2023-05-23 01:37:59,064:INFO: 	>>>  R@1: 41.9 - R@5: 69.3 - R@10: 80.2 - Median R: 2.0 - Mean R: 16.5
2023-05-23 01:37:59,065:INFO: Video-to-Text:
2023-05-23 01:37:59,065:INFO: 	>>>  V2T$R@1: 41.6 - V2T$R@5: 69.0 - V2T$R@10: 80.0 - V2T$Median R: 2.0 - V2T$Mean R: 11.9
2023-05-23 01:37:59,065:INFO: The best model is: /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.1, the R1 is: 41.9000
2023-05-23 01:38:19,845:INFO: Epoch: 3/5, Step: 50/5625, Lr: 0.000000065, Loss: 0.043166, Time/step: 0.206521
2023-05-23 01:39:00,419:INFO: Epoch: 3/5, Step: 150/5625, Lr: 0.000000065, Loss: 0.218455, Time/step: 0.405735
2023-05-23 01:39:41,009:INFO: Epoch: 3/5, Step: 250/5625, Lr: 0.000000064, Loss: 0.062240, Time/step: 0.405900
2023-05-23 01:40:21,601:INFO: Epoch: 3/5, Step: 350/5625, Lr: 0.000000064, Loss: 0.210228, Time/step: 0.405917
2023-05-23 01:41:02,193:INFO: Epoch: 3/5, Step: 450/5625, Lr: 0.000000063, Loss: 0.322749, Time/step: 0.405913
2023-05-23 01:41:42,799:INFO: Epoch: 3/5, Step: 550/5625, Lr: 0.000000063, Loss: 0.078543, Time/step: 0.406054
2023-05-23 01:42:23,404:INFO: Epoch: 3/5, Step: 650/5625, Lr: 0.000000062, Loss: 0.019625, Time/step: 0.406049
2023-05-23 01:43:04,020:INFO: Epoch: 3/5, Step: 750/5625, Lr: 0.000000061, Loss: 0.214575, Time/step: 0.406156
2023-05-23 01:43:44,628:INFO: Epoch: 3/5, Step: 850/5625, Lr: 0.000000061, Loss: 0.147171, Time/step: 0.406072
2023-05-23 01:44:25,236:INFO: Epoch: 3/5, Step: 950/5625, Lr: 0.000000060, Loss: 0.086219, Time/step: 0.406084
2023-05-23 01:45:05,831:INFO: Epoch: 3/5, Step: 1050/5625, Lr: 0.000000060, Loss: 0.331875, Time/step: 0.405938
2023-05-23 01:45:46,461:INFO: Epoch: 3/5, Step: 1150/5625, Lr: 0.000000059, Loss: 0.135480, Time/step: 0.406297
2023-05-23 01:46:27,088:INFO: Epoch: 3/5, Step: 1250/5625, Lr: 0.000000059, Loss: 0.152818, Time/step: 0.406264
2023-05-23 01:47:07,733:INFO: Epoch: 3/5, Step: 1350/5625, Lr: 0.000000058, Loss: 0.165842, Time/step: 0.406449
2023-05-23 01:47:48,405:INFO: Epoch: 3/5, Step: 1450/5625, Lr: 0.000000058, Loss: 0.248875, Time/step: 0.406721
2023-05-23 01:48:29,076:INFO: Epoch: 3/5, Step: 1550/5625, Lr: 0.000000057, Loss: 0.044600, Time/step: 0.406698
2023-05-23 01:49:09,733:INFO: Epoch: 3/5, Step: 1650/5625, Lr: 0.000000056, Loss: 0.142001, Time/step: 0.406565
2023-05-23 01:49:50,419:INFO: Epoch: 3/5, Step: 1750/5625, Lr: 0.000000056, Loss: 0.039244, Time/step: 0.406859
2023-05-23 01:50:31,081:INFO: Epoch: 3/5, Step: 1850/5625, Lr: 0.000000055, Loss: 0.204731, Time/step: 0.406622
2023-05-23 01:51:11,743:INFO: Epoch: 3/5, Step: 1950/5625, Lr: 0.000000055, Loss: 0.154741, Time/step: 0.406611
2023-05-23 01:51:52,426:INFO: Epoch: 3/5, Step: 2050/5625, Lr: 0.000000054, Loss: 0.119521, Time/step: 0.406830
2023-05-23 01:52:33,091:INFO: Epoch: 3/5, Step: 2150/5625, Lr: 0.000000054, Loss: 0.145314, Time/step: 0.406640
2023-05-23 01:53:13,755:INFO: Epoch: 3/5, Step: 2250/5625, Lr: 0.000000053, Loss: 0.167300, Time/step: 0.406635
2023-05-23 01:53:54,434:INFO: Epoch: 3/5, Step: 2350/5625, Lr: 0.000000053, Loss: 0.173138, Time/step: 0.406784
2023-05-23 01:54:35,100:INFO: Epoch: 3/5, Step: 2450/5625, Lr: 0.000000052, Loss: 0.286121, Time/step: 0.406658
2023-05-23 01:55:15,758:INFO: Epoch: 3/5, Step: 2550/5625, Lr: 0.000000051, Loss: 0.428270, Time/step: 0.406580
2023-05-23 01:55:56,398:INFO: Epoch: 3/5, Step: 2650/5625, Lr: 0.000000051, Loss: 0.187377, Time/step: 0.406390
2023-05-23 01:56:37,070:INFO: Epoch: 3/5, Step: 2750/5625, Lr: 0.000000050, Loss: 0.291121, Time/step: 0.406719
2023-05-23 01:57:17,732:INFO: Epoch: 3/5, Step: 2850/5625, Lr: 0.000000050, Loss: 0.232650, Time/step: 0.406613
2023-05-23 01:57:58,390:INFO: Epoch: 3/5, Step: 2950/5625, Lr: 0.000000049, Loss: 0.222031, Time/step: 0.406582
2023-05-23 01:58:39,056:INFO: Epoch: 3/5, Step: 3050/5625, Lr: 0.000000049, Loss: 0.095834, Time/step: 0.406649
2023-05-23 01:59:19,700:INFO: Epoch: 3/5, Step: 3150/5625, Lr: 0.000000048, Loss: 0.178051, Time/step: 0.406441
2023-05-23 02:00:00,369:INFO: Epoch: 3/5, Step: 3250/5625, Lr: 0.000000048, Loss: 0.236033, Time/step: 0.406685
2023-05-23 02:00:41,032:INFO: Epoch: 3/5, Step: 3350/5625, Lr: 0.000000047, Loss: 0.201367, Time/step: 0.406624
2023-05-23 02:01:21,701:INFO: Epoch: 3/5, Step: 3450/5625, Lr: 0.000000046, Loss: 0.075787, Time/step: 0.406684
2023-05-23 02:02:02,353:INFO: Epoch: 3/5, Step: 3550/5625, Lr: 0.000000046, Loss: 0.145136, Time/step: 0.406519
2023-05-23 02:02:43,001:INFO: Epoch: 3/5, Step: 3650/5625, Lr: 0.000000045, Loss: 0.231086, Time/step: 0.406474
2023-05-23 02:03:23,679:INFO: Epoch: 3/5, Step: 3750/5625, Lr: 0.000000045, Loss: 0.123185, Time/step: 0.406772
2023-05-23 02:04:04,356:INFO: Epoch: 3/5, Step: 3850/5625, Lr: 0.000000044, Loss: 0.176989, Time/step: 0.406765
2023-05-23 02:04:45,015:INFO: Epoch: 3/5, Step: 3950/5625, Lr: 0.000000044, Loss: 0.200257, Time/step: 0.406589
2023-05-23 02:05:25,679:INFO: Epoch: 3/5, Step: 4050/5625, Lr: 0.000000043, Loss: 0.213217, Time/step: 0.406639
2023-05-23 02:06:06,366:INFO: Epoch: 3/5, Step: 4150/5625, Lr: 0.000000043, Loss: 0.206599, Time/step: 0.406864
2023-05-23 02:06:47,042:INFO: Epoch: 3/5, Step: 4250/5625, Lr: 0.000000042, Loss: 0.065304, Time/step: 0.406752
2023-05-23 02:07:27,728:INFO: Epoch: 3/5, Step: 4350/5625, Lr: 0.000000041, Loss: 0.125438, Time/step: 0.406853
2023-05-23 02:08:08,381:INFO: Epoch: 3/5, Step: 4450/5625, Lr: 0.000000041, Loss: 0.241392, Time/step: 0.406525
2023-05-23 02:08:49,054:INFO: Epoch: 3/5, Step: 4550/5625, Lr: 0.000000040, Loss: 0.211413, Time/step: 0.406732
2023-05-23 02:09:29,727:INFO: Epoch: 3/5, Step: 4650/5625, Lr: 0.000000040, Loss: 0.080926, Time/step: 0.406727
2023-05-23 02:10:10,382:INFO: Epoch: 3/5, Step: 4750/5625, Lr: 0.000000039, Loss: 0.143913, Time/step: 0.406543
2023-05-23 02:10:51,052:INFO: Epoch: 3/5, Step: 4850/5625, Lr: 0.000000039, Loss: 0.053017, Time/step: 0.406694
2023-05-23 02:11:31,724:INFO: Epoch: 3/5, Step: 4950/5625, Lr: 0.000000038, Loss: 0.235562, Time/step: 0.406721
2023-05-23 02:12:12,388:INFO: Epoch: 3/5, Step: 5050/5625, Lr: 0.000000038, Loss: 0.199498, Time/step: 0.406630
2023-05-23 02:12:53,046:INFO: Epoch: 3/5, Step: 5150/5625, Lr: 0.000000037, Loss: 0.171946, Time/step: 0.406575
2023-05-23 02:13:33,714:INFO: Epoch: 3/5, Step: 5250/5625, Lr: 0.000000037, Loss: 0.171738, Time/step: 0.406676
2023-05-23 02:14:14,367:INFO: Epoch: 3/5, Step: 5350/5625, Lr: 0.000000036, Loss: 0.100293, Time/step: 0.406528
2023-05-23 02:14:55,028:INFO: Epoch: 3/5, Step: 5450/5625, Lr: 0.000000035, Loss: 0.213586, Time/step: 0.406609
2023-05-23 02:15:35,677:INFO: Epoch: 3/5, Step: 5550/5625, Lr: 0.000000035, Loss: 0.294587, Time/step: 0.406484
2023-05-23 02:16:06,232:INFO: Epoch 3/5 Finished, Train Loss: 0.190569
2023-05-23 02:16:08,969:INFO: Model saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.2
2023-05-23 02:16:08,969:INFO: Optimizer saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_opt.bin.2
2023-05-23 02:16:08,969:INFO: Eval on val dataset
2023-05-23 02:16:18,086:INFO: sim matrix size: 1000, 1000
2023-05-23 02:16:18,147:INFO: 	 Length-T: 1000, Length-V:1000
2023-05-23 02:16:18,148:INFO: Text-to-Video:
2023-05-23 02:16:18,148:INFO: 	>>>  R@1: 41.4 - R@5: 68.9 - R@10: 80.0 - Median R: 2.0 - Mean R: 15.8
2023-05-23 02:16:18,148:INFO: Video-to-Text:
2023-05-23 02:16:18,148:INFO: 	>>>  V2T$R@1: 42.5 - V2T$R@5: 68.7 - V2T$R@10: 79.8 - V2T$Median R: 2.0 - V2T$Mean R: 11.9
2023-05-23 02:16:18,148:INFO: The best model is: /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.1, the R1 is: 41.9000
2023-05-23 02:16:28,711:INFO: Epoch: 4/5, Step: 25/5625, Lr: 0.000000034, Loss: 0.117836, Time/step: 0.104351
2023-05-23 02:17:09,158:INFO: Epoch: 4/5, Step: 125/5625, Lr: 0.000000034, Loss: 0.091498, Time/step: 0.404460
2023-05-23 02:17:49,625:INFO: Epoch: 4/5, Step: 225/5625, Lr: 0.000000033, Loss: 0.096654, Time/step: 0.404674
2023-05-23 02:18:30,120:INFO: Epoch: 4/5, Step: 325/5625, Lr: 0.000000033, Loss: 0.281720, Time/step: 0.404937
2023-05-23 02:19:10,613:INFO: Epoch: 4/5, Step: 425/5625, Lr: 0.000000032, Loss: 0.285068, Time/step: 0.404929
2023-05-23 02:19:51,127:INFO: Epoch: 4/5, Step: 525/5625, Lr: 0.000000032, Loss: 0.071400, Time/step: 0.405135
2023-05-23 02:20:31,632:INFO: Epoch: 4/5, Step: 625/5625, Lr: 0.000000031, Loss: 0.072616, Time/step: 0.405045
2023-05-23 02:21:12,137:INFO: Epoch: 4/5, Step: 725/5625, Lr: 0.000000031, Loss: 0.051789, Time/step: 0.405049
2023-05-23 02:21:52,616:INFO: Epoch: 4/5, Step: 825/5625, Lr: 0.000000030, Loss: 0.403196, Time/step: 0.404784
2023-05-23 02:22:33,105:INFO: Epoch: 4/5, Step: 925/5625, Lr: 0.000000030, Loss: 0.046820, Time/step: 0.404890
2023-05-23 02:23:13,590:INFO: Epoch: 4/5, Step: 1025/5625, Lr: 0.000000029, Loss: 0.249051, Time/step: 0.404840
2023-05-23 02:23:54,108:INFO: Epoch: 4/5, Step: 1125/5625, Lr: 0.000000029, Loss: 0.073458, Time/step: 0.405173
2023-05-23 02:24:34,622:INFO: Epoch: 4/5, Step: 1225/5625, Lr: 0.000000028, Loss: 0.202367, Time/step: 0.405139
2023-05-23 02:25:15,140:INFO: Epoch: 4/5, Step: 1325/5625, Lr: 0.000000028, Loss: 0.194617, Time/step: 0.405179
2023-05-23 02:25:55,667:INFO: Epoch: 4/5, Step: 1425/5625, Lr: 0.000000027, Loss: 0.079612, Time/step: 0.405267
2023-05-23 02:26:36,215:INFO: Epoch: 4/5, Step: 1525/5625, Lr: 0.000000027, Loss: 0.087704, Time/step: 0.405473
2023-05-23 02:27:16,766:INFO: Epoch: 4/5, Step: 1625/5625, Lr: 0.000000026, Loss: 0.194394, Time/step: 0.405505
2023-05-23 02:27:57,319:INFO: Epoch: 4/5, Step: 1725/5625, Lr: 0.000000026, Loss: 0.358130, Time/step: 0.405528
2023-05-23 02:28:37,863:INFO: Epoch: 4/5, Step: 1825/5625, Lr: 0.000000025, Loss: 0.045613, Time/step: 0.405437
2023-05-23 02:29:18,412:INFO: Epoch: 4/5, Step: 1925/5625, Lr: 0.000000025, Loss: 0.159667, Time/step: 0.405485
2023-05-23 02:29:58,958:INFO: Epoch: 4/5, Step: 2025/5625, Lr: 0.000000024, Loss: 0.197021, Time/step: 0.405456
2023-05-23 02:30:39,530:INFO: Epoch: 4/5, Step: 2125/5625, Lr: 0.000000024, Loss: 0.069618, Time/step: 0.405715
2023-05-23 02:31:20,092:INFO: Epoch: 4/5, Step: 2225/5625, Lr: 0.000000023, Loss: 0.150356, Time/step: 0.405621
2023-05-23 02:32:00,643:INFO: Epoch: 4/5, Step: 2325/5625, Lr: 0.000000023, Loss: 0.139770, Time/step: 0.405508
2023-05-23 02:32:41,204:INFO: Epoch: 4/5, Step: 2425/5625, Lr: 0.000000022, Loss: 0.220407, Time/step: 0.405604
2023-05-23 02:33:21,763:INFO: Epoch: 4/5, Step: 2525/5625, Lr: 0.000000022, Loss: 0.185540, Time/step: 0.405581
2023-05-23 02:34:02,318:INFO: Epoch: 4/5, Step: 2625/5625, Lr: 0.000000021, Loss: 0.364976, Time/step: 0.405553
2023-05-23 02:34:42,880:INFO: Epoch: 4/5, Step: 2725/5625, Lr: 0.000000021, Loss: 0.078626, Time/step: 0.405608
2023-05-23 02:35:23,439:INFO: Epoch: 4/5, Step: 2825/5625, Lr: 0.000000021, Loss: 0.091724, Time/step: 0.405588
2023-05-23 02:36:03,986:INFO: Epoch: 4/5, Step: 2925/5625, Lr: 0.000000020, Loss: 0.159044, Time/step: 0.405463
2023-05-23 02:36:44,543:INFO: Epoch: 4/5, Step: 3025/5625, Lr: 0.000000020, Loss: 0.031798, Time/step: 0.405567
2023-05-23 02:37:25,115:INFO: Epoch: 4/5, Step: 3125/5625, Lr: 0.000000019, Loss: 0.052370, Time/step: 0.405719
2023-05-23 02:38:05,707:INFO: Epoch: 4/5, Step: 3225/5625, Lr: 0.000000019, Loss: 0.164095, Time/step: 0.405921
2023-05-23 02:38:46,292:INFO: Epoch: 4/5, Step: 3325/5625, Lr: 0.000000018, Loss: 0.202801, Time/step: 0.405842
2023-05-23 02:39:26,880:INFO: Epoch: 4/5, Step: 3425/5625, Lr: 0.000000018, Loss: 0.223406, Time/step: 0.405880
2023-05-23 02:40:07,445:INFO: Epoch: 4/5, Step: 3525/5625, Lr: 0.000000017, Loss: 0.168091, Time/step: 0.405648
2023-05-23 02:40:48,017:INFO: Epoch: 4/5, Step: 3625/5625, Lr: 0.000000017, Loss: 0.110847, Time/step: 0.405707
2023-05-23 02:41:28,588:INFO: Epoch: 4/5, Step: 3725/5625, Lr: 0.000000017, Loss: 0.091192, Time/step: 0.405714
2023-05-23 02:42:09,167:INFO: Epoch: 4/5, Step: 3825/5625, Lr: 0.000000016, Loss: 0.175172, Time/step: 0.405786
2023-05-23 02:42:49,777:INFO: Epoch: 4/5, Step: 3925/5625, Lr: 0.000000016, Loss: 0.125609, Time/step: 0.406093
2023-05-23 02:43:30,397:INFO: Epoch: 4/5, Step: 4025/5625, Lr: 0.000000015, Loss: 0.054794, Time/step: 0.406191
2023-05-23 02:44:10,957:INFO: Epoch: 4/5, Step: 4125/5625, Lr: 0.000000015, Loss: 0.310784, Time/step: 0.405599
2023-05-23 02:44:51,543:INFO: Epoch: 4/5, Step: 4225/5625, Lr: 0.000000015, Loss: 0.069895, Time/step: 0.405857
2023-05-23 02:45:32,142:INFO: Epoch: 4/5, Step: 4325/5625, Lr: 0.000000014, Loss: 0.176611, Time/step: 0.405990
2023-05-23 02:46:12,723:INFO: Epoch: 4/5, Step: 4425/5625, Lr: 0.000000014, Loss: 0.085737, Time/step: 0.405808
2023-05-23 02:46:53,278:INFO: Epoch: 4/5, Step: 4525/5625, Lr: 0.000000013, Loss: 0.076186, Time/step: 0.405545
2023-05-23 02:47:33,857:INFO: Epoch: 4/5, Step: 4625/5625, Lr: 0.000000013, Loss: 0.092138, Time/step: 0.405781
2023-05-23 02:48:14,452:INFO: Epoch: 4/5, Step: 4725/5625, Lr: 0.000000013, Loss: 0.084617, Time/step: 0.405949
2023-05-23 02:48:55,017:INFO: Epoch: 4/5, Step: 4825/5625, Lr: 0.000000012, Loss: 0.132134, Time/step: 0.405643
2023-05-23 02:49:35,620:INFO: Epoch: 4/5, Step: 4925/5625, Lr: 0.000000012, Loss: 0.089429, Time/step: 0.406029
2023-05-23 02:50:16,222:INFO: Epoch: 4/5, Step: 5025/5625, Lr: 0.000000012, Loss: 0.107456, Time/step: 0.406015
2023-05-23 02:50:56,819:INFO: Epoch: 4/5, Step: 5125/5625, Lr: 0.000000011, Loss: 0.218036, Time/step: 0.405968
2023-05-23 02:51:37,403:INFO: Epoch: 4/5, Step: 5225/5625, Lr: 0.000000011, Loss: 0.147628, Time/step: 0.405833
2023-05-23 02:52:17,981:INFO: Epoch: 4/5, Step: 5325/5625, Lr: 0.000000011, Loss: 0.160348, Time/step: 0.405774
2023-05-23 02:52:58,529:INFO: Epoch: 4/5, Step: 5425/5625, Lr: 0.000000010, Loss: 0.097960, Time/step: 0.405476
2023-05-23 02:53:39,102:INFO: Epoch: 4/5, Step: 5525/5625, Lr: 0.000000010, Loss: 0.089590, Time/step: 0.405727
2023-05-23 02:54:19,667:INFO: Epoch: 4/5, Step: 5625/5625, Lr: 0.000000010, Loss: 0.030570, Time/step: 0.405650
2023-05-23 02:54:19,723:INFO: Epoch 4/5 Finished, Train Loss: 0.145210
2023-05-23 02:54:22,439:INFO: Model saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.3
2023-05-23 02:54:22,439:INFO: Optimizer saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_opt.bin.3
2023-05-23 02:54:22,439:INFO: Eval on val dataset
2023-05-23 02:54:31,762:INFO: sim matrix size: 1000, 1000
2023-05-23 02:54:31,824:INFO: 	 Length-T: 1000, Length-V:1000
2023-05-23 02:54:31,824:INFO: Text-to-Video:
2023-05-23 02:54:31,824:INFO: 	>>>  R@1: 41.5 - R@5: 68.2 - R@10: 79.3 - Median R: 2.0 - Mean R: 16.3
2023-05-23 02:54:31,824:INFO: Video-to-Text:
2023-05-23 02:54:31,824:INFO: 	>>>  V2T$R@1: 42.7 - V2T$R@5: 68.1 - V2T$R@10: 79.5 - V2T$Median R: 2.0 - V2T$Mean R: 12.0
2023-05-23 02:54:31,824:INFO: The best model is: /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.1, the R1 is: 41.9000
2023-05-23 02:55:12,810:INFO: Epoch: 5/5, Step: 100/5625, Lr: 0.000000009, Loss: 0.213613, Time/step: 0.408575
2023-05-23 02:55:53,338:INFO: Epoch: 5/5, Step: 200/5625, Lr: 0.000000009, Loss: 0.138532, Time/step: 0.405277
2023-05-23 02:56:33,899:INFO: Epoch: 5/5, Step: 300/5625, Lr: 0.000000009, Loss: 0.057346, Time/step: 0.405612
2023-05-23 02:57:14,438:INFO: Epoch: 5/5, Step: 400/5625, Lr: 0.000000008, Loss: 0.026822, Time/step: 0.405385
2023-05-23 02:57:55,016:INFO: Epoch: 5/5, Step: 500/5625, Lr: 0.000000008, Loss: 0.296068, Time/step: 0.405774
2023-05-23 02:58:35,594:INFO: Epoch: 5/5, Step: 600/5625, Lr: 0.000000008, Loss: 0.117565, Time/step: 0.405780
2023-05-23 02:59:16,166:INFO: Epoch: 5/5, Step: 700/5625, Lr: 0.000000007, Loss: 0.037577, Time/step: 0.405712
2023-05-23 02:59:56,739:INFO: Epoch: 5/5, Step: 800/5625, Lr: 0.000000007, Loss: 0.146701, Time/step: 0.405727
2023-05-23 03:00:37,334:INFO: Epoch: 5/5, Step: 900/5625, Lr: 0.000000007, Loss: 0.087294, Time/step: 0.405944
2023-05-23 03:01:17,922:INFO: Epoch: 5/5, Step: 1000/5625, Lr: 0.000000007, Loss: 0.077599, Time/step: 0.405882
2023-05-23 03:01:58,506:INFO: Epoch: 5/5, Step: 1100/5625, Lr: 0.000000006, Loss: 0.182658, Time/step: 0.405832
2023-05-23 03:02:39,112:INFO: Epoch: 5/5, Step: 1200/5625, Lr: 0.000000006, Loss: 0.160167, Time/step: 0.406057
2023-05-23 03:03:19,720:INFO: Epoch: 5/5, Step: 1300/5625, Lr: 0.000000006, Loss: 0.173989, Time/step: 0.406073
2023-05-23 03:04:00,317:INFO: Epoch: 5/5, Step: 1400/5625, Lr: 0.000000005, Loss: 0.127534, Time/step: 0.405972
2023-05-23 03:04:40,938:INFO: Epoch: 5/5, Step: 1500/5625, Lr: 0.000000005, Loss: 0.077774, Time/step: 0.406207
2023-05-23 03:05:21,575:INFO: Epoch: 5/5, Step: 1600/5625, Lr: 0.000000005, Loss: 0.311431, Time/step: 0.406363
2023-05-23 03:06:02,231:INFO: Epoch: 5/5, Step: 1700/5625, Lr: 0.000000005, Loss: 0.219619, Time/step: 0.406553
2023-05-23 03:06:42,866:INFO: Epoch: 5/5, Step: 1800/5625, Lr: 0.000000004, Loss: 0.095969, Time/step: 0.406350
2023-05-23 03:07:23,509:INFO: Epoch: 5/5, Step: 1900/5625, Lr: 0.000000004, Loss: 0.113983, Time/step: 0.406423
2023-05-23 03:08:04,141:INFO: Epoch: 5/5, Step: 2000/5625, Lr: 0.000000004, Loss: 0.408143, Time/step: 0.406310
2023-05-23 03:08:44,799:INFO: Epoch: 5/5, Step: 2100/5625, Lr: 0.000000004, Loss: 0.118713, Time/step: 0.406579
2023-05-23 03:09:25,452:INFO: Epoch: 5/5, Step: 2200/5625, Lr: 0.000000004, Loss: 0.086922, Time/step: 0.406530
2023-05-23 03:10:06,124:INFO: Epoch: 5/5, Step: 2300/5625, Lr: 0.000000003, Loss: 0.043595, Time/step: 0.406710
2023-05-23 03:10:46,763:INFO: Epoch: 5/5, Step: 2400/5625, Lr: 0.000000003, Loss: 0.016630, Time/step: 0.406393
2023-05-23 03:11:27,406:INFO: Epoch: 5/5, Step: 2500/5625, Lr: 0.000000003, Loss: 0.160724, Time/step: 0.406423
2023-05-23 03:12:08,062:INFO: Epoch: 5/5, Step: 2600/5625, Lr: 0.000000003, Loss: 0.090032, Time/step: 0.406559
2023-05-23 03:12:48,723:INFO: Epoch: 5/5, Step: 2700/5625, Lr: 0.000000003, Loss: 0.290771, Time/step: 0.406600
2023-05-23 03:13:29,373:INFO: Epoch: 5/5, Step: 2800/5625, Lr: 0.000000002, Loss: 0.134135, Time/step: 0.406497
2023-05-23 03:14:10,014:INFO: Epoch: 5/5, Step: 2900/5625, Lr: 0.000000002, Loss: 0.054047, Time/step: 0.406409
2023-05-23 03:14:50,666:INFO: Epoch: 5/5, Step: 3000/5625, Lr: 0.000000002, Loss: 0.134534, Time/step: 0.406515
2023-05-23 03:15:31,320:INFO: Epoch: 5/5, Step: 3100/5625, Lr: 0.000000002, Loss: 0.196911, Time/step: 0.406541
2023-05-23 03:16:11,986:INFO: Epoch: 5/5, Step: 3200/5625, Lr: 0.000000002, Loss: 0.060118, Time/step: 0.406657
2023-05-23 03:16:52,617:INFO: Epoch: 5/5, Step: 3300/5625, Lr: 0.000000002, Loss: 0.131089, Time/step: 0.406301
2023-05-23 03:17:33,247:INFO: Epoch: 5/5, Step: 3400/5625, Lr: 0.000000002, Loss: 0.013348, Time/step: 0.406296
2023-05-23 03:18:13,893:INFO: Epoch: 5/5, Step: 3500/5625, Lr: 0.000000001, Loss: 0.197770, Time/step: 0.406463
2023-05-23 03:18:54,517:INFO: Epoch: 5/5, Step: 3600/5625, Lr: 0.000000001, Loss: 0.070826, Time/step: 0.406227
2023-05-23 03:19:35,167:INFO: Epoch: 5/5, Step: 3700/5625, Lr: 0.000000001, Loss: 0.033481, Time/step: 0.406505
2023-05-23 03:20:15,803:INFO: Epoch: 5/5, Step: 3800/5625, Lr: 0.000000001, Loss: 0.089935, Time/step: 0.406352
2023-05-23 03:20:56,445:INFO: Epoch: 5/5, Step: 3900/5625, Lr: 0.000000001, Loss: 0.218224, Time/step: 0.406419
2023-05-23 03:21:37,077:INFO: Epoch: 5/5, Step: 4000/5625, Lr: 0.000000001, Loss: 0.156899, Time/step: 0.406314
2023-05-23 03:22:17,721:INFO: Epoch: 5/5, Step: 4100/5625, Lr: 0.000000001, Loss: 0.123270, Time/step: 0.406433
2023-05-23 03:22:58,368:INFO: Epoch: 5/5, Step: 4200/5625, Lr: 0.000000001, Loss: 0.146585, Time/step: 0.406472
2023-05-23 03:23:39,002:INFO: Epoch: 5/5, Step: 4300/5625, Lr: 0.000000001, Loss: 0.148717, Time/step: 0.406334
2023-05-23 03:24:19,631:INFO: Epoch: 5/5, Step: 4400/5625, Lr: 0.000000000, Loss: 0.073985, Time/step: 0.406286
2023-05-23 03:25:00,277:INFO: Epoch: 5/5, Step: 4500/5625, Lr: 0.000000000, Loss: 0.008156, Time/step: 0.406455
2023-05-23 03:25:40,890:INFO: Epoch: 5/5, Step: 4600/5625, Lr: 0.000000000, Loss: 0.237058, Time/step: 0.406127
2023-05-23 03:26:21,552:INFO: Epoch: 5/5, Step: 4700/5625, Lr: 0.000000000, Loss: 0.031099, Time/step: 0.406618
2023-05-23 03:27:02,194:INFO: Epoch: 5/5, Step: 4800/5625, Lr: 0.000000000, Loss: 0.180168, Time/step: 0.406409
2023-05-23 03:27:42,821:INFO: Epoch: 5/5, Step: 4900/5625, Lr: 0.000000000, Loss: 0.084138, Time/step: 0.406272
2023-05-23 03:28:23,479:INFO: Epoch: 5/5, Step: 5000/5625, Lr: 0.000000000, Loss: 0.034243, Time/step: 0.406569
2023-05-23 03:29:04,090:INFO: Epoch: 5/5, Step: 5100/5625, Lr: 0.000000000, Loss: 0.214152, Time/step: 0.406109
2023-05-23 03:29:44,697:INFO: Epoch: 5/5, Step: 5200/5625, Lr: 0.000000000, Loss: 0.171553, Time/step: 0.406065
2023-05-23 03:30:25,334:INFO: Epoch: 5/5, Step: 5300/5625, Lr: 0.000000000, Loss: 0.144369, Time/step: 0.406366
2023-05-23 03:31:05,967:INFO: Epoch: 5/5, Step: 5400/5625, Lr: 0.000000000, Loss: 0.086729, Time/step: 0.406325
2023-05-23 03:31:46,583:INFO: Epoch: 5/5, Step: 5500/5625, Lr: 0.000000000, Loss: 0.178663, Time/step: 0.406163
2023-05-23 03:32:27,203:INFO: Epoch: 5/5, Step: 5600/5625, Lr: 0.000000000, Loss: 0.321344, Time/step: 0.406195
2023-05-23 03:32:37,422:INFO: Epoch 5/5 Finished, Train Loss: 0.133558
2023-05-23 03:32:40,126:INFO: Model saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.4
2023-05-23 03:32:40,126:INFO: Optimizer saved to /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_opt.bin.4
2023-05-23 03:32:40,126:INFO: Eval on val dataset
2023-05-23 03:32:49,335:INFO: sim matrix size: 1000, 1000
2023-05-23 03:32:49,395:INFO: 	 Length-T: 1000, Length-V:1000
2023-05-23 03:32:49,396:INFO: Text-to-Video:
2023-05-23 03:32:49,396:INFO: 	>>>  R@1: 41.4 - R@5: 68.3 - R@10: 79.2 - Median R: 2.0 - Mean R: 16.3
2023-05-23 03:32:49,396:INFO: Video-to-Text:
2023-05-23 03:32:49,396:INFO: 	>>>  V2T$R@1: 42.7 - V2T$R@5: 68.2 - V2T$R@10: 79.4 - V2T$Median R: 2.0 - V2T$Mean R: 12.0
2023-05-23 03:32:49,397:INFO: The best model is: /media/dc/新加卷/dc/hyt/hyt2/res/outputs/4_1_CLIP4Clip_uniform/pytorch_model.bin.1, the R1 is: 41.9000